<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>TCP on Distributed</title>
    <link>https://www6v.github.io/www6vDistributed/categories/TCP/</link>
    <description>Recent content in TCP on Distributed</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sun, 09 Aug 2020 12:03:41 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vDistributed/categories/TCP/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>TIME_WAIT和优化</title>
      <link>https://www6v.github.io/www6vDistributed/docs/%E6%80%A7%E8%83%BD/%E7%BD%91%E7%BB%9C/tcpTimewait/</link>
      <pubDate>Sun, 09 Aug 2020 12:03:41 +0000</pubDate>
      <guid>https://www6v.github.io/www6vDistributed/docs/%E6%80%A7%E8%83%BD/%E7%BD%91%E7%BB%9C/tcpTimewait/</guid>
      <description>故障现象 # 让我们先从一例线上故障说起。在一次升级线上应用服务之后，我们发现该服务的可用性变 得时好时坏，一段时间可以对外提供服务，一段时间突然又不可以， 大家都百思不得其解。 运维同学登录到服务所在的主机上，使用 netstat 命令查看后才发现，主机上有成千上万处 于 TIME_WAIT 状态的连接。&#xA;经过层层剖析后，我们发现罪魁祸首就是 TIME_WAIT。为什么呢？我们这个应用服务需要 通过发起 TCP 连接对外提供服务。每个连接会占用一个本地端口，当在高并发的情况下， TIME_WAIT 状态的连接过多，多到把本机可用的端口耗尽，应用服务对外表现的症状，就 是不能正常工作了。当过了一段时间之后，处于 TIME_WAIT 的连接被系统回收并关闭 后，释放出本地端口可供使用，应用服务对外表现为，可以正常工作。这样周而复始，便会 出现了一会儿不可以，过一两分钟又可以正常工作的现象。&#xA;TIME_WAIT概念和作用 # 概念 # 只有发起连接终止的一方会进入 TIME_WAIT 状态。 TIME_WAIT停留持续时间是固定的，是最长分节生命期 MSL（maximumsegment lifetime）的两倍（2MSL）。Linux系统里有一个硬编码的字段，名称为TCP_IMEWAIT_LEN，其值为 60 秒。也就是说，Linux 系统停留在 TIME_WAIT 的时间为固定的 60 秒。 作用 # 这样做是为了确保最后的 ACK 能让被动关闭方接收（可能丢失ACK后重传ACK），从而帮助其正常关闭。 第二个理由和连接“化身”和报文迷走有关系，为了让旧连接的重复分节在网络中自然消失。 TIME_WAIT的副作用和优化 # TCP和TIME_WAIT # 当连接的一方主动关闭连接，在接收到对端的 FIN 报文之后，主动关闭连接的一方会在 TIME_WAIT 这个状态里停留一段时间，这个时间大约为 2MSL。&#xA;主动关闭连接的一方可以是客户端，也可以是服务端； 副作用1 主动关闭连接的一方 是客户端； 副作用2 主动关闭连接的一方 是服务端；&#xA;副作用和优化 # 副作用1 对端口资源的占用，一个 TCP 连接至少消耗一个本地端口</description>
    </item>
    <item>
      <title>TCP流控和拥塞控制</title>
      <link>https://www6v.github.io/www6vDistributed/docs/%E6%80%A7%E8%83%BD/%E7%BD%91%E7%BB%9C/tcpUdpControlCongestion/</link>
      <pubDate>Wed, 07 Aug 2019 13:52:09 +0000</pubDate>
      <guid>https://www6v.github.io/www6vDistributed/docs/%E6%80%A7%E8%83%BD/%E7%BD%91%E7%BB%9C/tcpUdpControlCongestion/</guid>
      <description>&#xA;TCP拥塞控制 # TCP拥塞控制-快速重传 # 快速重传 # 拥塞窗口是为了怕把网络塞满，在出现丢包的时候减少发送速度. 滑动窗口就是为了怕把接收方塞满，而控制发送速度.&#xA;参考: # TCP 的那些事儿（下） （四）：网络性能排查之TCP重传与重复ACK 怎么让不可靠的UDP可靠？ 趣谈Linux操作系统 - 45-发送网络包（上）：如何表达我们想让合作伙伴做什么？ </description>
    </item>
  </channel>
</rss>
