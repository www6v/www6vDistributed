<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>容量保障 on Distributed</title>
    <link>https://www6v.github.io/www6vDistributed/categories/%E5%AE%B9%E9%87%8F%E4%BF%9D%E9%9A%9C/</link>
    <description>Recent content in 容量保障 on Distributed</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sun, 13 Mar 2022 15:58:58 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vDistributed/categories/%E5%AE%B9%E9%87%8F%E4%BF%9D%E9%9A%9C/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>容量保障与全链路压测</title>
      <link>https://www6v.github.io/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/%E5%AE%B9%E9%87%8F%E4%BF%9D%E9%9A%9C/capacityGuarantee/</link>
      <pubDate>Sun, 13 Mar 2022 15:58:58 +0000</pubDate>
      <guid>https://www6v.github.io/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/%E5%AE%B9%E9%87%8F%E4%BF%9D%E9%9A%9C/capacityGuarantee/</guid>
      <description>容量保障Overview[2] # 大促容量保障的三项重点工作： 大促流量预估 大促容量测试 大促容量保障预案 容量预测 # 容量预测[1] # 我首先给出了**“皮尔逊相关系数”**这个工具，对服务 TPS 和 CPU 利用率之间的相关度进 行了定量分析，根据相关度的强弱，分别采取不同策略。其中，重点讲到了在两者弱相关 时的应对策略，如果能够穷举出尽可能多的相关特征，可以通过特征选取的方式对服务进 行画像，提升预测准确率；如果特征非常难找，那么可以依靠概率表的方式曲线救国。&#xA;随着服务不断迭代，容量也在不断变化，我与你分析的第二个问题，就是如何平衡好服务 迭代和容量预测频率的关系。根据服务发布窗口（或其他变更时间点）建立滑动窗口机 制，既保证了在服务变更后能够尽快地更新模型，又不至于带来大量的计算量，是一个不 错的实践方式。&#xA;业务场景变化也会导致容量变化，针对这个问题，我结合之前提到的全链路压测工作，通 过建立全链路压测和容量预测双向校准的机制，提前对变化的业务场景进行预测，识别容 量风险。&#xA;全链路压测 # 核心功能[4] # 压测-部署架构 # 施压机的分布[3]&#xA;大部分仍然是跟线上系统在同机房内，少量会在公有云节点上 以将全球（主要是国内）的 CDN 节点作为施压机 更加真实地模拟真实用户从全球节点进入的真实访问流量 成本过高，技术条件和细节难 压测的读写流量[3]&#xA;读流量 写流量 对压测的写请求做专门的标记。 当请求要写数据库时，由分布式数据库的中间件框架中的逻辑来判断这个请求是否是压测请求，如果是压测写请求则路由到对应的影子库中，而不是直接写到线上正式的库中。 改造 全链路压测[5] # 数据隔离 物理隔离 vs. 逻辑隔离 [见表1] 中间件改造 eg. MQ改造 ​ Producer: 判断请求带压测标识，转换到数据体（msg）中 ​ Consumer: 判断数据（msg）中有压测标识， 恢复压测标识至请求中 应用服务改造 绕开限制逻辑 数据隔离前置 Mock 逻辑 ​ 表1 逻辑隔离 vs.</description>
    </item>
  </channel>
</rss>
