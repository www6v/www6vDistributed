[{"id":0,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/cleanCode/","title":"整洁架构 *","section":"应用架构","content":"\n整洁架构 Clean Architecture # 核心观点 [7][8]\n不与框架绑定\njava-spring, Quarkus 可测试\nmock- gomock, Testify 不与UI绑定 不与数据库绑定\nDDD 中的Repo 不依赖任何外部代理 Go的实现 [9][10]\n参考 # 《24 直播：框架之上的业务分层》 体系课_Go高级工程师实战营(完结)\nThe Clean Architecture\nclean-architecture-go-v2 git\ngo-clean-arch\nGolang 简洁架构实战 未\nGo整洁架构实践 未\n"},{"id":1,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/SRE/SRE/sre/","title":"SRE 总结","section":"SRE","content":"\nSRE [1] # SRE = PE（Production Engineer） + 工具平台开发 + 稳定性平台开发 工具平台团队，负责效能工具的研发，比如实现 CMDB、运维自动化、持续交付流水线以 及部分技术运营报表的实现，为基础运维和应用运维提供效率平台支持。 稳定性平台团队，负责稳定性保障相关的标准和平台，比如监控、服务治理相关的限流降 级、全链路跟踪、容量压测和规划。 组织架构图 [1] # 故障复盘 [2] # 黄金三问 第一问：故障原因有哪些？ 第二问：我们做什么，怎么做才能确保下次不会再出现类似故障？ 第三问：当时如果我们做了什么，可以用更短的时间恢复业务？ 故障判定的三原则 健壮性原则。 第三方默认无责。 分段判定原则。 5W 分析法 Google SRE Principle [5] # 运营是软件问题 服务水平目标SLO 减少琐事 用自动化的方式减少琐事 自动化 ** 统一环境， IaC CaC， ** IaC: Terraform, Ansible, Pulumi 生产环境中进行测试 统一 版本管理，制品库，cmdb 可观测性 降低失败成本 复盘 从失败中学习 共享所有权 个人安全，责任共担 拥抱风险 + 错误预算 Google SRE 实践总结 [5] # 确保长期关注研发工作 在保障SLO的前提下最大化迭代速度 监控系统 + insight，根因 应急事件处理 变更管理 ITIL 需求预测和容量规划 + AIOps 资源部署 效率与性能 参考 # 《09｜案例：互联网典型的SRE组织架构是怎样的？》 赵成 《08｜故障复盘：黄金三问与判定三原则》 赵成 xxx SRE 的工作介绍 SRE大佬 未 SRE核心概念与可观测性介绍 中国DevOps社区 刘峰 +《SRE google 运维解密》\n《SRE google 运维解密》读书笔记 （一） 未 《SRE google 运维解密》读书笔记 （二） 未 《SRE google 运维解密》读书笔记 （三） 《SRE google 运维解密》读书笔记 （四） "},{"id":2,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/distributedSystemPattern/","title":"分布式系统 模式 *","section":"系统架构","content":"\nFault Tolerant Consensus # 英文 中文 Paxos Paxos Quorum *** Quorum Generation Clock *** 世代时钟（Generation Clock） Pattern Sequence for implementing replicated log # 英文 中文 Example \u0026amp; 扩展 Replicated Log *** 复制日志（Replicated Log） Raft Write-Ahead Log *** 预写日志（Write-Ahead Log） MySQL redo log\nHBase WAL\nRedis aof Low-Water Mark *** 低水位标记（Low-Water Mark） High-Water Mark *** 高水位标记（High-Water Mark） kafka HW {% post_link \u0026lsquo;streamingFlinkWatermarkWindow\u0026rsquo; %} HeartBeat *** 心跳（HeartBeat） {% post_link \u0026lsquo;crashDetect\u0026rsquo; %} Leader and Followers ***\nleader election 领导者和追随者（Leader and Followers） Go to Page Go to Page {% post_link \u0026lsquo;zookeeperZab\u0026rsquo; %}\nes 选主[todo] Follower Reads *** 追随者读取（Follower Reads） Segmented Log *** 分段日志（Segmented Log） Kafka segment Request Pipeline 请求管道（Request Pipeline） Singular Update Queue 单一更新队列（Singular Update Queue） Single Socket Channel 单一 Socket 通道（Single Socket Channel） Atomic Commit # 英文 中文 Example \u0026amp; 扩展 Two Phase Commit 两阶段提交（Two Phase Commit） 两阶段变种 Fixed Partitions / Key-Range Partitions / Kubernetes or Kafka Control Plane # 英文 中文 Example \u0026amp; 扩展 Lease *** 租约（Lease） {% post_link \u0026lsquo;crashDetect\u0026rsquo; %} ETCD Lease K8s Lease\nEureka Lease State Watch *** 状态监控（State Watch） Idempotent Receiver 幂等接收者（Idempotent Receiver） Logical Timestamp usage # 英文 中文 Example \u0026amp; 扩展 Lamport Clock Lamport 时钟（Lamport Clock） Versioned Value *** 有版本的值（Versioned Values） tikv mvcc[TSO]\nCockroachDB mvcc[HLC]\nHybrid Clock *** 混合时钟（Hybrid Clock） CockroachDB Gossip Dissemination *** Gossip 传播（Gossip Dissemination） Redis Gossip Consistent Core 一致性内核（Consistent Core） Version Vector 版本向量（Version Vector） Others # Request Batch Clock-Bound Wait Emergent Leader Request Waiting List Self # Example \u0026amp; 扩展 time wheel {% post_link \u0026rsquo;timedTask\u0026rsquo; %} 参考 # Patterns of Distributed Systems 《分布式系统模式》中文版 Go to Page self "},{"id":3,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/api_design/","title":"OpenAPI 设计","section":"应用架构","content":"\nREST API 设计 规范 # OpenAPI Specification 业界标准\nA Visual Guide to What\u0026rsquo;s New in Swagger 3.0 OpenAPI Specification - Version 3.0.2 Google API Design Guide\n谷歌API设计指南 API 设计模式 # RPC ROA(Rest-Oriented Architecture) 通常RESTful风格对API设计者的要求是比较高的，主要的难点在于面向资源设计要求开发者事先做好规划，将后端数据模型与API服务模型相匹配。\n面向资源设计API # 资源模型 资源分类管理 资源关系 ECS TAG功能详解 资源组\n服务容错处理 # 同步请求的Timeout[2] 异步请求方式 错误码\nTagResources 错误码 参考: # 云服务OpenAPI的7大挑战，架构师如何应对？ 阿里技术 虚明 超时和重试总结 self "},{"id":4,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/feed/","title":"Feed流 总结","section":"系统设计","content":"\nFeed总结 # 消息同步模型 # 消息同步模型- 左:BCDEF的发件箱，右:A的收件箱\n基于Timeline的消息库设计 # 基于Timeline的消息库设计 - 上：用于写扩散消息同步，下：全量历史消息，读扩散消息同步\n推拉结合 # 基于用户类型的Timeline推拉结合(读扩散/写扩散混合) - 上面是发布流程，下面是阅读流程\n读扩散 vs 写扩散 # 拉模式(读扩散) 推模式(写扩散)[推荐使用] 发布 个人页Timeline（发件箱） 粉丝的关注页（收件箱） 阅读 所有关注者的个人页Timeline 自己的关注页Timeline 网络最大开销 用户刷新时 发布Feed时 读写放大 放大读：读写比例到1万:1 放大写减少读：读写比例到50:50 优点 只要写一次 接收端消息同步逻辑会非常简单 缺点、副作用 1.读被大大的放大\n2.响应时间长 消息写入会被放大， 数据会极大膨胀， 针对副作用的优化-推拉结合 1.大V采用拉模式，普通用户使用推模式\n2.对活跃粉丝采用推模式，非活跃粉丝采用拉模式 场景 # 场景 Timeline IM单聊 三个Timeline IM群聊 1 + N个Timeline 朋友圈 1 + N个Timeline 微博 大V发一条微博就是 1 + M个Timeline（M \u0026laquo; N，N是粉丝数） Rank # 参考 # feed流拉取，读扩散，究竟是啥？ 如何打造千万级Feed流系统 TableStore Timeline：轻松构建千万级IM和Feed流系统 现代IM系统中消息推送和存储架构的实现 Feed流系统设计-总纲 未 "},{"id":5,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/middleStage/","title":"中台战略","section":"系统架构","content":"\n目录 # 中台全景图 # 中台全景图[6] 中台和微服务[4] 中台\n业务中台 核心业务层 技术中台 iaas+paas 数据中台 理念 阿里提出： 大中台， 小前台\n业务中台 # 业务中台 # 阿里共享服务 [6] 淘宝 天猫 共享 商品，交易，店铺等服务 京东业务中台 [5] 技术中台 [1][2][3] # 参考 # 《企业IT架构转型之道-阿里巴巴中台战略思想与架构实战》 钟华 全面异步化：淘宝反应式架构升级探索 淘宝应用柔性架构的探索 《微服务架构核心20讲-如何理解阿里巴巴提出的微服务中台战略？》 杨波 中小型电商相当适配：京东商城系统架构设计原则精炼 "},{"id":6,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/SRE/%E6%95%85%E9%9A%9C%E6%A8%A1%E5%9E%8B/faultModel1/","title":"故障模型-应用层","section":"故障模型","content":"\n故障模型-Application \u0026amp; Data # OOM # + 堆内 【10】\r- PermGen\r+ 原因：反射类多\r+ 解决\r先jmap，后btrace【11、12 case2】\r- heap\r解决：对比Fullgc后的相同对象的数量、大小\r+ 堆外Native\r参考 Go to Page self 如何排查Java内存泄露(内附各种排查工具介绍) 不闻 生产环境下持久带满导致FullGC，如何跟踪 Go to Page self 应用性能变差 # + 原因\r- 锁 - heap\r+ fullgc后没有空间\r原因：内存泄露\r工具：heap dump\r+ fullgc后有空间\r解决：设置门槛，过滤大量短生命周期对象【12 case1，13】\r- gc停顿长\r解决：【12 case3，13】\r参考 Go to Page self Go to Page self 听阿里巴巴JVM工程师为你分析常见Java故障案例 *** Load过高 # + cpu load高【13】\r- 启动阶段\r原因：JIT编译器\r解决：分层编译\r- 运行阶段\r原因：有热点方法\r工具：MAT\r- 解决\r工具【8、9】\r数据收集 4、5\r参考\n不正当使用HashMap导致cpu 100%的问题追究 王宏江\n一个由正则表达式引发的血案 CPU飚高 vjtools useful-scripts 听阿里巴巴JVM工程师为你分析常见Java故障案例 *** 进程Hang # 进程被杀， JVM crash # + 原因：\rJNI Unsafe\r+ 工具：core dump\r异常 # + 启动异常\r+ 心跳异常\rCI/CD # + 环境错误\r+ 部署包错误\r+ 配置错误，误删\r系统单点 # + 原因: 设计问题\r+ 解决：服务去状态，多实例部署\r异步阻塞同步 # 依赖超时，依赖异常 # + 解决【15,16】\r- 区分强弱依赖\r- 压测 + 容量规划\r- 熔断 \u0026amp;\u0026amp; 降级\r参考 Go to Page self {% post_link \u0026lsquo;stability\u0026rsquo; %} self 业务线程池满 # + 最佳实践:【4】\r+ 案例：【14】\r- 问题：使用blockingQueue.put\r- 解决：blockingQueue.offer（time超时机制）+限制队列长度\r- 最佳实践：生产上线程池的core Size和poolSize设置的一样，请求不在队列里排队\r参考 Go to Page self 从一个故障说说Java的三个BlockingQueue 阿里毕玄 流控不合理 # 其他-数据收集 # + 1.Heap dump\r+ 2. GC 日志\r+ 3. core dump\r+ 4. 线程stack\r+ 5.os进程信息\r参考 # 大纲 # 超全总结 | 阿里如何应对电商故障？神秘演练细节曝光 阿里巴巴 周洋 Application \u0026amp; Data # 如何检测 Web 服务请求丢失问题 Nginx tracing + Tomcat tracing 系统中的故障场景建模 应用层 大方法 codecache "},{"id":7,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/designPrinciple/","title":"设计原则","section":"设计原则","content":"\nLaw 定律 # 奥卡姆剃刀原理 *** # 如果对于一个现象有好几种解释, 那么最简单的解释往往是最正确的.\n排队理论 # Little\u0026rsquo;s 定律 -\u0026gt; 应用 ： 线程池中多线程个数的确定。\n康威定律 # organizations which design systems \u0026hellip; are constrained to produce designs which are copies of the communication structures of these organizations\n衍生: 1).DDD context 2).微服务模块划分 Amdahl定律, 通用扩展定律(Universal Scalability Law, USL) # CAP/BASE # 复杂度 简化本质复杂度，消除偶发复杂性. # 有三个问题可能会产生偶发复杂度。\n第一个：由于日程或其他外部压力而导致临时大量削减代码。 第二个是复制。 第三个诱因是不可逆性，您做出的无法逆转的所有决定都将最终导致某种程度的偶发复杂度。 架构师： 去熵， 去复杂度。\n原则 Principle # SOLID *** # 开闭原则 [3] 对于扩展是开放的（Open for extension） 对于修改是关闭的（Closed for modification） Happy path \u0026amp; Sad path 代码执行路径： happy path 和 sad path分离。 # 测试用例： happy path用例。 sad path用例， 使用@Exception（Junit4）， fail（JUnit3）。\n笛米特法则 # 只和最亲密的朋友讲话(talk only to your immediate friends). 任何对象都不需要知道与之交互的对象的任何细节.\nMongoDB设计哲学 # Databases are specializing – the “one size fits all” approach no longer applies.\n“KISS”原则 - Keep it simple and stupid # 衍生: Rob Pike - Simplicity is Complicated\nRule of least power（够用就好）的原则。 # 这个原则是由 WWW 发明者 Tim Berners-Lee 提出的，它被广泛用于指导各种 W3C 标准制定\n参考 # 对开发人员有用的定律、理论、原则和模式 *** 滴滴杜欢：大型微服务框架设计实践 {% post_link \u0026lsquo;designOCPspi\u0026rsquo; %} self "},{"id":8,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/stability/","title":"稳定性总结","section":"SRE","content":"\n关键词: 容量规划, 压测, 强弱依赖, 关键词: 故障模型, 故障演练, 故障注入\n稳定性总结 # 参考： # 超全总结 | 阿里如何应对电商故障？神秘演练细节曝光 阿里巴巴-周洋（花名中亭） 故障注入， 故障演练 稳定性思考-强弱依赖 阿里中间件团队博客 稳定性思考-强弱依赖2 阿里中间件团队博客 中间件技术及双十一实践·稳定性平台篇 阿里中间件（Aliware） 强弱依赖， 容量规划 \u0026laquo;尽在双11阿里巴巴技术演进与超越\u0026raquo; "},{"id":9,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/designOCPspi/","title":"开闭原则 - SPI","section":"设计原则","content":"\n开闭原则（Open Closed Principle） # open for extension, but closed for modification\n开闭原则实现 - SPI # SPI Java SPI Dubbo SPI ExtensionLoader Spring SPI @FunctionalInterface @Order(Ordered.LOWEST_PRECEDENCE) public interface MyBeanPostProcessor extends BeanPostProcessor { // define your methods here } 参考 # Java SPI机制以及和Dubbo/Spring SPI对比 面试官问烂的Dubbo中SPI机制的源码解析 *** 未\n源码级深度理解 Java SPI 未\n剖析 SPI 在 Spring 中的应用 未\n"},{"id":10,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/systemDesign/","title":"系统设计 总结","section":"系统设计","content":"\n特点 设计 Feed流 读写比例100:1\n消息必达性要求高 非稳定的账号关系 {% post_link \u0026lsquo;feed\u0026rsquo; %} 秒杀系统 {% post_link \u0026lsquo;secKillSummary\u0026rsquo; %} {% post_link \u0026lsquo;seckill\u0026rsquo; %} 计数系统[2] 数据量巨大\n访问量大，性能要求高\n对于可用性、数字的准确性要求高 + 方案1 数据库 + 缓存 -\u0026gt; 按照 weibo_id做分库分表。数据库和缓存之间无法保证数据的一致性 [2]\n+方案2 全部写入redis -\u0026gt; MQ异步写，批量合并写。redis昂贵， 存储优化，冷热分离 [2] 抢红包系统[3] 交易类信息:红包发、抢、拆、详情列表 展示类信息: 收红包列表、发红包列表 + 架构设计，理论基础是快慢分离。红包入账是一个分布事务，属于慢接口。而拆红包凭证落地则速度快 [3]\n+ 高并发 set化， 局部化-控制同一红包并发个数 排行榜系统 参考 # 计数系统 # 计数系统架构实践一次搞定 | 架构师之路 未\n\u0026laquo;37丨计数系统设计（一）：面对海量数据的计数器要如何做？\u0026raquo; 唐扬\n红包系统 # 揭秘微信红包架构、抢红包算法和高并发和降级方案 微信红包\n揭秘微信红包：架构、抢红包算法、高并发和降级方案\n微信红包后台系统设计 未\n微信高并发资金交易系统设计方案\u0026mdash;\u0026ndash;百亿红包背后的技术支撑\nSET化、请求排队串行化、双维度分库表 未\n"},{"id":11,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/disaggregationOfComputeAndStorage/","title":"存算分离-数据应用","section":"系统架构","content":"\n存算分离 存算一体 RMDB MySQL Cluster MySQL Group Replicatoin(MGR) NewSQL,HTAP TiDB[tidb,tikv] , openGaussDB[2], CockroachDB? Aurora[10], PolarDB[3], PGXC风格[1] 大数据 clickhouse, hbase ES MQ Pulsar kafka, rocketmq 文件系统 Ceph[PG, ODS], PolarFS[5] KV GaussDB(for Redis) [4], Codis redis Cluster 其他 serverless[FasS, BaaS] PolarDB [7] 基于Redo Log物理复制实现的一写多读共享存储集群 参考 # [1] 《04 | 架构风格：NewSQL和PGXC到底有啥不一样？》 王磊\n[2] opengauss系统架构\n[3] PolarDB Serverless: A Cloud Native Database for Disaggregated Data Centers\n[4] GaussDB(for Redis)揭秘：Redis存算分离架构最全解析\n[5] 阿里推出PolarFS分布式文件系统，存储与计算分开！附论文\n[6] Go to Page self\n[7] \u0026laquo;云原生数据库 原理与实践\u0026raquo; 5.2 5.3\n[10] VERBITSKI A，GUPTA A，SAHA D，et al. Amazon aurora：design considerations for high throughput cloud-native relational databases ［C］. Proceedings of the 2017 ACM International Conference on Man⁃ agement of Data，2017 ：1041-1052.\n"},{"id":12,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/SRE/%E6%95%85%E9%9A%9C%E6%A8%A1%E5%9E%8B/faultModel2/","title":"故障模型-中间件层","section":"故障模型","content":"\n故障模型-中间件层 # 故障模型-Runtime\u0026amp;Middleware\u0026amp;OS 负载均衡失效 数据库 数据库热点 数据库连接满 数据库宕机 数据库同步延迟 数据库主备延迟【参考2】 缓存 缓存热点【参考1】 缓存限流 OS资源 CPU抢占 案例 : HashMap并发访问，CPU100%【参考1】 案例：正则表达式回溯，CPU100% 内存抢占 案例：OOM killer 上下文切换 参考 # 大纲 # 超全总结 | 阿里如何应对电商故障？神秘演练细节曝光 阿里巴巴 周洋 Runtime \u0026amp; Middleware \u0026amp; OS # Go to Page self\nUCloud高可用数据库UDB主从复制延时的解决\n"},{"id":13,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/cqrs/","title":"CQRS 简介和案例分析","section":"应用架构","content":"\nCQRS全称是指Command Query ResponsibilitySeparation.CQRS的核心是一个简单的概念, 使用一个模型来读信息, 使用另一个模型来更新信息. 它是CQS原理在各个软件领域中的应用而产生的一种模式. CQRS把整个系统分成两个部分: 命令部分和查询部分. Command部分关注更新, Query部分关注读取.\n其实你可能早就接触过CQRS相关的概念,熟悉数据库的读者不会对索引陌生. Query部分:如果数据表有索引, 读数据表更加的快速. Command部分:如果数据表有index,update表时需要更新index, 所以update更加的慢.\n本文主要从CQRS在高伸缩性系统和领域驱动设计(DDD)两方面的应用阐述其优势。\nCQRS的出现有以下两种驱动力\n多参与者协作的环境 多个参与者会使用和修改相同的数据集. 参与者可以是行为人用户, 或者是软件. 数据总是过时的 在多协作的环境中, 数据一旦显示给了一个用户, 相同的数据可能已经被其它的参与者修改了, 说明数据已经过时了. 在哲学领域有一个命题, 你是否能踏进同一条河两次? 在多协作的环境中也有类似的问题, 你看到的数据总是过时的. 案例: 在查询出还有电影场次后, 你开始填自己的记录信息, 这时可能别人已经订购了你已经选择的座位, 或者这个时候, 一个事件到达银行说你信用卡有拖欠, 但最后你提交了这次订购，结果订购失败。\nCQRS与模型 # 在与command模型的交互中产生了事件, 顺序事件的累积可以捕获状态的所有变化, 这种交互模式称为事件源(Event Sourcing) .\n事件源(Event Sourcing)使得系统有了审计的功能, 回放事件可以使系统恢复到某个时间点的状态. 事件源(Event Sourcing)使command部分引入了异步的机制, 队列中的消息不需要马上处理, event handler可以异步的消费事件.当commands部分产生错误后, 直接向客户端回个错误并不友好, 这时可以引入回滚和重试机制. 在系统恢复正常之后, 队列中的消息重新发送并且用户接受到确认.\nQuery与Command两种行为的分离使得两个服务公用模型的分离也成为自然(图2)。单一模型(图1)分离成了两个模型:查询模型和命令模型.接口相应也分离成查询接口和命令接口. 客户端通过命令接口路由变化信息到命令模型. 查询模型和命令模型之间往往通过异步方式同步数据. 客户端通过查询接口读取查询模型以得到更新后的数据.\n但是模型在上下文中孤立的存在并不多见，更多模型之间会有相互的渗透，融合(图3)。共享内核表示了命令模型和查询模型之间重合的部分. (DDD) 在DDD领域中, 通用子系统可以代表更通用的服务. 在存储系统中, 通用子系统代表了在存储介质上的数据结构的融合, 公用.\n结合Event Souring 和模型共享内核来了解一下通用存储引擎的设计思路\n案例: BigTable和Cassandra的通用存储引擎\n数据写入时需要先写操作日志, 操作日志可以看成是Event Souring的持久化保存.成功后应用到内存中的MemTable中. 当内存中的MemTable达到一定大小, 需要将MemTable dump到磁盘中生成SSTable.由于数据同时存在MemTable和可能多个SSTable中, 读取操作需要按老到新合并SSTable和内存中的MemTable数据. 可以看到写操作对应的命令模型是MemTable, 读操作对应的查询模型是MemTable和多个SSTable,MemTable在读写时成为了共享模型, 以达到’提高写性能, 亦不降低读性能’的目的.\nCQRS与RESTFUL # 在REST风格的系统中, 资源动词, 名词, 表现三个维度上的分离, 形成了资源行为(统一接口), 资源状态, 资源表现形式. REST的6个约束中包括统一接口, 能够使客户端和服务端独立的演化。统一接口包括PUT, GET, POST等Http方法. PUT, POST类的接口可以归为command部分, GET 类的接口可以归为query部分. CQRS使得资源行为维度能够再分, 形成对服务层, 模型层, 数据存取层(DAO), 数据源层的纵向切分, 形成command和query两个子系统.REST统一接口是系统的水平接口，CQRS可以看成是系统的垂直接口。 在系统中, C和Q的分离可以看成是对系统中最粗粒度层次的划分.\n案例:Facebook缓存架构\n• 整体REST架构分成PUT(Query部分), POST(Command部分)两个部分. • Cache分Page cache, fragment cache, row cache, vector Cache, cache命中率见图。 • Page Cache和Fragment cache存放了API各种请求格式的数据，包括4种资源表现形式 XML, JSON, RSS, ATOM。 • 发表Tweets是先放入Kestrel, 再异步处理，Kestrel用的也是memcached协议。Kestrel可以看成Event Souring, Vector Cache是Command部分和Query部分之间的共享模型.\nCQRS与一致性 # 根据弱CAP原理，在分布式系统中，往往需要达到(一致性, 可用性,分区容忍性)三者的平衡，增强其中的一方就会削弱另外两方。在分布式系统中, P总是需要保证的, 所以需要在C和A之间做取舍. CQRS中的S(分离)隐喻了P, 即分区容忍性.\n贯彻CQRS的系统通过多种方式来实现各种级别的一致性，其中包括MS, MM(MMS, MMM), 两阶段提交, Paxos\n强一致性：假如A 先写入了一个值到存储系统，存储系统保证后续A，B,C的读取操作都将返回最新值。 弱一致性：假如A先写入了一个值到存储系统，存储系统不能保证后续A，B，C的读取操作能读取到最新值。 最终一致性：最终一致性是弱一致性的一种特例。假如A首先write了一个值到存储系统，存储系统保证如果在A，B，C后续读取之前没有其它写操作更新同样的值的话，最终所有的读取操作都会读取到A写入的最新值。\nMS # 在分布式系统中，通过读写多个数据副本来做到读写分离。 MS方式中, Master会承担起写请求(Command部分)的负载, Slave会承担起读请求(Query部分)的负载. 多个slave副本通过同步, 异步, 半同步的方式达到与Master数据的一致性.异步同步对延时和吞吐量这两个性能指标有好处. 在读多写少的系统中, 增加读的副本可以相对廉价的提高Query部分(读请求端)的水平可伸缩性. 如果有大量突增请求, 可以相应调高读的副本数.\u0026ndash;query部分的可伸缩性\nMM # Multi-master指一个系统存在多个master, 每个master都具有read-write能力，可以根据时间戳或业务逻辑合并版本。具备最终一致性。\n案例 BigTable: 同一个时刻同一个tablet只能被一台Tablet Server服务. 强一致性的分布式索引. GFS: MS实现的弱一致性分布式存储系统. Dynamo和Cassandra: MM实现的具备最终一致性的存储系统. 可能出现同一个key被多台机器操作的情况.多台机器上执行的顺序是无法保证的. 需要依赖基于vector lock的冲突合并方法解决冲突. 默认的解决方案是”last write wins”, 即在读的时候合并多个写者产生的多个版本的数据.\n\u0026ndash;To do\nCQRS与数据存储 # Command部分:　相对关注事务处理,持久化为关系结构数据. 在数据库中, 使用第3范式. Query部分: 相对关注性能. 使用反范式的方式来最小化数据的级联. 在数据库中, 可以使用第一范式, 也可以结合使用nosql技术.\n案例:　Mysql + Redies混合存储(sql + nosql混合存储)\nMySQL把数据同步到NoSQL中,这种架构适用于需要把数据同步到多种类型的存储中。 Nosql通过装做是mysql的slave, 从mysql同步数据.MySQL到NoSQL同步的实现可以使用MySQL UDF函数，MySQL binlog的解析来实现。\nCQRS与分布式事务 # 两阶段提交是实现分布式事务的常用方式, 协议比较通用. 但两阶段提交中所有事务序列化的通过master coordinator, 是吞吐率和延迟的杀手.\nCQRS是完全建立在BASE(Basic Availability, Soft-state, Eventual consistency)事务基础上的. 在CQRS实现中, 通过降低对写端的压力, 减少锁的竞争和死锁的可能, 来增加写端的性能.各种实现方式会有自定义的协议, 相对于两阶段提交灵活但不够通用.\n在对性能要求不高的系统中, 应该采用两阶段提交加快开发. 在对性能要求不是很高系统中, 应该考虑采用消息队列.\n案例1:　ebay分布式事务 消息队列上的CQRS + 消息应用状态表\n更新业务表A\n更新业务表B的事件放入消息队列\n提交事务1(包括步骤1, 2)\n查询队列中的消息, 更新业务表B.\n插入消息应用状态表message_applied\n提交事务2(包括步骤5, 6)\n如果上述事务成功之后, dequeue message\n删除消息应用状态表中的事件\n在关注第2点和第4点之后, 可以看到队列的插入(command)和查询(query) 放在了两个事务中.\n案例2: 淘宝分布式事务 日志表上的CQRS + 去重表\n更新业务表A\n更新业务表B的事件放入日志表, 并自动生成一个唯一的transactionID。\n提交事务1(包括步骤1, 2)\n消息中间件保证从主机1上读取更新业务表B的事件和transactionID, 并且这个消息路由到主机2.\n更新业务表B\n将transactionID插入去重表\n提交事务2(包括步骤5, 6)\n这两个实现方式, 它们有之间的共性, 就是有一个元素被分步骤的使用了CQRS.在案例1中是消息队列，案例2中是日志表. 从CQRS的角度看, 这两种实现方式没有本质的区别, 方式2可以看成是方式1的变体.\n案例2中的去重表等价于案例1中的消息应用状态表. 案例2看似复杂, 多了步骤4, 实际是保持事务1,2同步临界区的最小化, 等于是把案例1中查询队列中的消息(步骤4)剥离出事务, 防止不必要的查询错误导致回滚整个事务. 案例1的事务1牵涉到了业务表A和队列的混合型业务事务,实现复杂。案例2的两个事务都是数据库的系统事务, 可以使用两阶段提交, 相对通用. 案例2的去重表没有删除过, 能保证最终的消息都是已经成功的事务.案例1 有dequeue message步骤, 并在事务外, 在出现故障后, message_applied会留下一些垃圾内容. \u0026ndash;\nCQRS与CDN # 在大规模web站点中, 动态数据和静态数据(图片)的分离是优化的通用策略. CQRS能够在Query端做到极致的优化, 例如缓存, 分区, 备份(replication), 分布式的CDN. CDN是一种离用户相对近的边缘缓存, 能提高用户体验.\n***案例: *** 淘宝CDN\nCQRS原理 # CQRS来源于Bertrand Meyer提出的CQS原理。CQS原理从OOP中推导出来，大致是说如果你返回一个值你就不能改变状态。如果你改变了状态，你的返回值必须是void类型的。\nCQS原理其实在很多领域都有应用, 包括OOP中字段的的setter/getter,Java String(copy-on-write), Java thread(ConcurrentHashMap 读写分离锁), snapshot(copy-on-write), 数据库索引, 数据库sql(DML, DDL)中都可以看到CQS原理的影子.\n小结: # CQS中的分离(S)粒度,小到对象状态的setter和getter方法, 大到子系统的形成. 就如Greg谈到的， CQRS本身是个简单的小模式，有趣的是在结合两个服务时所要考虑的架构属性。 CQRS在复杂性管理和提高系统伸缩性有着独特的优势。\n参考: # clarified CQRS CQRS CQRS Documents by Greg Young NoSQL架构实践（一）——以NoSQL为辅 DDD \u0026ndash; domain driven design (共享内存) \u0026ndash; Even Eric Rethinking architecture with CQRS Twitter架构图(cache篇) Event Sourcing \u0026ndash; Martin fowler "},{"id":14,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/available/","title":"高可用 Available","section":"系统架构","content":"\n目录 # 原理 # CAP # CP系统: hbase, zookeeper AP系统: cassandra, eureka\nnPRT公式 [1] # 可以推导出风险期望的公式 # 控制风险的4大因素（nPRT） # 减少风险数量，n 降低风险变故障的概率（即：增加风险变故障的难度），P 减小故障影响范围，R 缩短故障影响时长，T 高可用架构设计的7大核心原则 # 少依赖原则：能不依赖的，尽可能不依赖，越少越好（n） 弱依赖原则：一定要依赖的，尽可能弱依赖，越弱越好（P） 分散原则：鸡蛋不要放一个篮子，分散风险（R） 均衡原则：均匀分散风险，避免不均衡（R） 隔离原则：控制风险不扩散，不放大（R） 无单点原则：要有冗余或其他版本,做到有路可退（T） 自我保护原则：少流血，牺牲一部分，保护另外一部分（P\u0026amp;R\u0026amp;T） 可用性 7 级图表 [成熟度] [5] # 当一个服务挂了的时候\n第一级：Crash with data corruption, destruction. 第二级：Crash with new data loss. 第三级：Crash without data loss. 数据高可用-冗余, destruction 测试 第四级：No crash, but with no or very limited service, low service quality.\n流控系统， eg. 秒杀流量漏斗 第五级：Partial or limited service, with good to medium service quality. 第六级：Failover with significant user visible delay, near full quality of service\n容灾，恢复慢 第七级：Failover with minimal to none user visible delay, near full quality\n异地容灾 高可用-研发 # 容量规划和评估 [7] # [chat] 容量规划和评估的概念和流程。 容量评估是评估系统需要应对的业务体量，包括请求量、高峰峰值等，可以根据历史数据或产品预估来进行。容量规划则是在系统设计时就要考虑容量问题，规划好系统能够抗多少的量级，涉及到系统架构设计和资源分配等问题。而性能压测则是为了确保容量规划的准确性，通过压测来测试系统的性能指标，如QPS和响应耗时，以确定系统是否能够承受实际业务流量。\n性能压测要关注的指标很多，但是重点要关注是两个指标，**一个是 QPS、一个是响应耗时，**要确保压测的结果符合预期。\nQPS 预估（漏斗型） [7] # [chat] QPS预估中的漏斗型预估方法。 漏斗型预估是根据请求的层面和模块来构建漏斗模型，预估每个层级的QPS量级，随着请求链路的下行，QPS量级会逐步减少。预估的层级包括服务、接口、分布式缓存等各个层面，最终构成完整的QPS漏斗模型。漏斗型预估方法可以帮助我们更准确地预估系统承载的QPS量级，从而做出更合理的容量规划和评估。\nQPS 预估（漏斗型）就是需要我们按照请求的层面和模块来构建我们的预估漏斗模型，然后预估好每一个层级的量级，包括但不限于从服务、接口、分布式缓存等各个层面来预估，最后构成我们完整的 QPS 漏斗模型。\n高可用-服务分层 # 分层解析 [6] # 接入层 [2] [R] # 地域\u0026amp;错误感知自动 failover 视 endpoint 健康度自动 failover 一定比例流量至其他可用区/地域，直至 endpoint 全部不健康时 100% 流量自动 failover 至其他可用区/地域。 地域感知流量分发 distribute eg. 上海一区和上海二区按照 80% 和 20% 的比例分发 服务层 应用层 [6] # 关注点 [7]\n无状态和负载均衡设计 弹性扩缩容设计 异步解耦和削峰设计（消息队列） 故障和容错设计 过载保护设计（限流、熔断、降级） 传统应用高可用\nCLB+CVM+AS 架构图 [pic] 应用实践 云原生应用部署\n涉及的产品 微服务平台 TSF API网关 TKE容器服务 云原生应用部署方案[pic] 应用的容灾设计 [pic 要重新看]\n单区域容灾 跨地域容灾 跨地域多活 业务拆分, 单元化部署 混合云部署 云上和IDC各部署一套完整的业务系统 异地多活set化部署 Unit由多个set组成 建议单写多读的架构 set不一定限制在一个机房，可跨机房、跨地域部署 系统中的高可用\nKubernetes 之 master高可用集群搭建 Redlock - redis分布式锁的高可用 百亿规模API网关服务Shepherd的设计与实现 服务隔离\n集群隔离 请求隔离 稳定性\n流量管控, 请求缓存, 超时管理, 熔断降级 中间件层 # kafka 高可用 zk高可用 系统中的高可用\n面试|图解 kafka 的高可用机制 isr 数据层 [3] [P] # 数据复制\n主从复制 同步复制，异步复制 复制日志的实现： 基于语句到复制， 基于wal的传输， 基于行的逻辑日志复制 eg. mysql， redis， hbase 复制滞后问题 多主复制 无主复制 一致性和共识 raft - etcd zab - zookeeper\n系统中的高可用\nMySQL 同步复制及高可用方案总结 MHA, MMM\n这可能是目前最全的Redis高可用技术解决方案总结 Master-slave, Cluster\n干货 | 阿里巴巴HBase高可用8年抗战回忆录\netcd - raft\n高可用-运营[7] # 灰度发布 # 监控+告警 # 安全性、防攻击设计 # 故障演练（混沌实验） # 接口拨测+巡检 # 参考 # 高可用的本质 云原生应用负载均衡系列 (2): 入口流量分发、容错与高可用调度 istio \u0026laquo;数据密集型应用系统设计\u0026raquo; 5章, 9章 \u0026laquo;亿级流量 网站架构核心技术\u0026raquo; 1.4 来自 Google 的高可用架构理念与实践 {% post_link \u0026rsquo;tencentTCP3\u0026rsquo; %} self 高可用架构和系统设计经验 腾讯 *** "},{"id":15,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/ddd/","title":"DDD  领域驱动设计","section":"应用架构","content":"\n目录 # 目标 # DDD的精髓是降低系统复杂度 规则 三个核心 # 统一语言 # 核心领域词汇表 统一语言重构迭代： 模型-》实现-》 重构-》 隐喻 -》 再到模型 命名规范 DSL - Domain Specific Language 领域划分 # 领域 子域 边界上下文（Bounded Context） 上下文映射（Context Mapping) 共享内核（Shared Kernel） 防腐层（Anti-Corruption）： 类似adaptor、facade， 对内部领域模型的隔离和屏蔽。 领域模型 # 抽象： 是从具体事物抽取、概括出它们共同的方面、本质属性与关系等。\n领域建模方法论： UML用例分析、 UML用例分析法 四色建模法 事件风暴\n模式 [3] # {% asset_img \u0026lsquo;ddd.png\u0026rsquo; %}\n实体 [4]\n可变性是实体的特点\n值对象 [4] 不变性是值对象的本质\nservice 领域服务是多个实体组合出来的一段业务逻辑\n聚合[5] 真实世界中整体与部分的关系 正是因为有这样的关系，在操作整体的时候，整体就封装了对部分的操作。 所谓的整体与部分的关系，就是当整体不存在时，部分就变得没有了意义。\n每个聚合对应一个Repo interface [7] 对聚合内的数据一致性负责[7] 聚合根 外部访问的唯一入口\n架构 # 六边形架构 # 又被称之为Ports and Adapters（端口和适配器架构）\n参考 # 《DDD（Domain Driven Design)的精髓》 直播+ppt 阿里张建飞 钉钉2020.05.21视频 xxx 领域驱动设计之领域模型 *** 《04 领域模型是如何指导程序设计的？》 DDD 微服务落地实战-拉钩专栏 《05 聚合、仓库与工厂：傻傻分不清楚》 DDD 微服务落地实战-拉钩专栏 {% post_link \u0026lsquo;DomainLogicAndSQL\u0026rsquo; %} self 资源 # 《实现领域驱动设计》 B *** 《领域专用语言实战》 B 没纸质 《领域驱动设计精粹》B 没纸质 《中台架构与实现 : 基于DDD和微服务》 B 没纸质 事件风暴和领域建模在阿里巴巴的落地实践 未 《DDD实战课》 《mksz541-DDD（领域驱动设计）思想解读及优秀实践~4》 V 《lg2061-DDD 微服务落地实战-拉钩专栏》 V *** 有代码 "},{"id":16,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/secKillSummary/","title":"秒杀系统总结","section":"系统设计","content":"\n参考： # 阿里大秒系统 秒杀系统架构优化思路 58沈剑 如何设计一个高可用、高并发秒杀系统 未 "},{"id":17,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/SRE/%E6%95%85%E9%9A%9C%E6%A8%A1%E5%9E%8B/faultModel3/","title":"故障模型-基础设施层","section":"故障模型","content":"\n故障模型-基础设施层 # 故障模型-Virtualization\u0026amp;Storage\u0026amp;Networking 服务器宕机\u0026amp;假死 断电 解决：异地多活 超卖 混和部署【3】 存储【2】 磁盘满，坏 不可写，不可读 网络【1】 网络抖动、丢包、超时 网卡满 DNS故障 断网 参考 # Virtualization \u0026amp; Storage \u0026amp; Networking # Kubernetes 网络疑难杂症排查分享 腾讯云 *** kubernetes 最佳实践：处理容器数据磁盘被写满 腾讯云 百度大规模战略性混部系统演进 "},{"id":18,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/ddd_practice/","title":"DDD-落地实战 Practice","section":"应用架构","content":"\nDDD 落地 # 基于DDD应用架构的核心 # 分离业务复杂度和技术复杂度\n设计思路 [4] # 贫血模型\n实现 业务逻辑放到Service中 缺点 [7] 业务逻辑被埋没在存储业务中 贫血模型的缺陷 [21] 无法保护模型对象的完整性和一致性 对象操作的可发现性极差 代码逻辑重复 代码的健壮性差 强依赖底层实现 99%的代码都是基于贫血模型 [21] 数据库思维 贫血模型“简单” 脚本思维 充血模型\n实现 业务逻辑放到领域对象中(实体对象中有实现方法) 开闭原则 保持了对象的封装性，使得领域模型在面临多态、继承等复杂结构时，易于变更 适用场景 类似继承、多态的情况 在软件设计的过程中需要将一些类型或者编码进行转换 更好地表现领域对象之间的关系 “聚合”，也就是在真实世界中那些代表整体与部分的事 比较\n贫血模型比充血模型更加简单易行 贫血模型 不需要 仓库、工厂、缓存，简单粗暴 充血模型需要更强的设计与协作能力 充血模型 需要开发人员有更强的OOA/D能力、分析业务、业务建模与设计能力 要有较强的团队协作能力 贫血模型 所有业务处理过程都交给Service完成 贫血模型更容易应对复杂的业务处理场景 分层 [2] # 用户接口层(Controller层) Application层 Domain层 Infrastructure层 代码分层 [2] # Interface\nassembler(DTO和领域对象的互转) dto facade（粗粒度的调用接口，将用户请求委派给一个或多个应用服务进行处理） Application\nevent（pub， sub）（事件处理相关的核心业务逻辑在领域层实现） service（应用服务） Domain\naggregate entity 聚合根 实体 值对象 工厂模式（Factory） event 事件实体以及与事件活动相关的业务逻辑代码 repository 所在聚合的查询或持久化领域对象的代码，通常包括仓储接口和仓储实现方法 Data Model只存在于数据层，而Domain Model在领域层，而链接了这两层的关键对象，就是Repository [7] service 领域服务是多个实体组合出来的一段业务逻辑 Infrastructure\nconfig Util（开发框架、消息、数据库、缓存、文件、总线、网关、第三方类库、通用算法等基础代码，） 项目代码[20] # 框架 # Axon Framework COLA [22] 参考 # 《13丨代码模型（上）：如何使用DDD设计微服务代码模型？》 欧创新 《04 领域模型是如何指导程序设计的？》 DDD 微服务落地实战-拉钩专栏 《24 直播：框架之上的业务分层》 体系课_Go高级工程师实战营(完结) 阿里技术专家详解DDD系列 第二讲 - 应用架构 refactor 之前的Transaction Script git refactor 之后的DDD 阿里技术专家详解DDD系列 第三讲 - Repository模式 COLA 4.0：应用架构的最佳实践 未 "},{"id":19,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/unifyModel/","title":"统一模型","section":"系统架构","content":"\n计算密集 # 计算密集 技术 产品 微服务 RPC(2th)Service Mesh(3th)\n多运行时(4th ) Dubboistio proxylessdaper 容器 编排 K8s Service Mesh Sidecar 控制面， 数据面 Envoy xDS 微软SMI 可观测 Tracing+Metric+Logs OpenTelemetry=\nOpenCensus+OpenTracing Sererless Sererless+ VMSererless+容器Sererless+服务Sererless+数据库 Ali ECSAli ECIFasSAurora，TiDB Cloud 数据密集 # 数据密集 技术 产品 消息队列 CloudEvent EventMesh 数据库 分离: 存算分离(资源伸缩)\n融合: HTAP(模型) TiDB(TiKV, TiFlash) ，PolarDB 大数据 流计算 Beam，Flink "},{"id":20,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/seckill/","title":"秒杀系统和商品详情页系统(培训讲义)","section":"系统设计","content":"\n秒杀系统和商品详情页系统(培训讲义)\n"},{"id":21,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/eai/","title":"应用集成方式","section":"应用架构","content":"\n企业应用之间主要有4种集成方式 # 文件传输 # 文件传输\n由各个应用产生文件, 其中包含提供其它应用使用的信息. 特征:　内部数据模式自由(schema free)\n优势: 1. 内部细节透明 2. 松耦合 3. 标准文件格式支持,如xml, json等.\n劣势: 1. 更新慢, 数据不同步, 数据过时 2. 数据不完整, 数据不完全正确时, 数据不一致问题解决困难.3. 产生大量小文件时, 昂贵且不可靠. 4. 语义不一致\n适用于批处理.\n共享数据库 # 共享数据库\n把应用的数据存储在一个共享数据库中来集成应用,　并定制数据库模式来处理不同应用的各种需求. 特征:　数据有模式(schema)\n优势: 1. 提供模型一致性 2. 通过事务管理数据一致性 3. 共享数据, 避免语义不一致问题.\n劣势: 1. 统一的数据库模式难设计. 模式改变,　应用也要改变. 2. 遗留系统很难提供一个可扩展的数据库模式供新的应用使用. 3.　性能瓶颈(单点访问) 4. 数据分布迁移困难. 5.应用和数据库紧耦合. 可作为新老系统的集成和改造的候选方案\n远程调用 # 远程调用 应用公开提供过程, 并能够被远程调用, 应用通过调用这些过程来执行操作并交换数据. 特征:　公开外部接口\n优势:　1. 提供语义一致性 2. 接口多样化, 有可兼容性并能扩展 3. 技术实现广泛, 如Java, .Net, CORBA, Web Services 4. 接口内部数据完整性和透明性.\n劣势:　１.　性能瓶颈,　不可靠(与本地访问相比)　２.　应用之间紧耦合, 可能会有时序上的耦合\n消息传递 # 消息传递 应用连接到一个公共的消息传递系统上,　并通过消息来交换数据和调用行为. 特征:　隐式调用, 完全隐藏接口\n优势:　1. 松耦合 2. 快速响应 3. 可靠 3. 通过消息转换解决语义不一致(实现方式: DDD 防腐层, ESB提供消息转换功能 )\n劣势: 1. 设计, 开发复杂(消息消费能力不够, 会引起消息的大量堆叠); 测试, 调试困难(可通过同步方式来测试) 2. 数据不完全同步 3. 有学习曲线\n权衡 # 耦合性和依赖性 # １.　消息传递和文件传输属于非直接耦合,　耦合性最低.\n２.　远程调用属于数据耦合,　耦合性次之.\n３.　共享数据库属于内容耦合,　耦合性最高.\n使用远程调用或共享数据库的应用之间是强依赖的关系\n使用消息传递或文件传输的应用之间是弱依赖的关系.\n修改性 # 共享数据库中的表结构一旦修改, 应用多少会做一些修改加以应对, 有的甚至是对整个应用的改造.\n数据格式 # 文件传输保持所产生的文件内容及格式不变就可以, 以文件作为公共接口, 内部格式可以不段变化. 远程调用, 共享数据库使用接口参数定义数据格式, 内部数据格式对外不可见. 共享数据库中数据格式的演化和扩展性相对于其它方式最弱.\n数据新鲜度 # 文件传输使用低频率的大文件传输会造成过时的信息, 有时可以容忍数据不一致, 但也可能造成灾难. 消息传递通过频繁和立即的发送数据来提高数据的新鲜度.\n数据序列化 # 数据需要序列化,反序列化. 格式可以是文本或者是二进制的. 传输的格式可以是json, xml或者PB.\n问题的解决 # 共享数据库和远程调用解决了文件传输语义不一致的问题. 消息传递相对于远程调用,共享数据库提高了性能和可靠性.\n模式之间的组合 # 消息传递和消息传递的组合(全异步化 SEDA) 消息传递和远程调用的组合(Half-Sync/Half-Async 半同步/半异步 POSA4) 远程调用和共享数据库的组合 ![(2) 变体, 应用之间有同步数据的问题, 实时或非实时同步]( \u0026ldquo;(2) 变体, 应用之间有同步数据的问题, 实时或非实时同步\u0026rdquo;)\n小结 # 4种方式, 每种模式都建立在前一种模式的基础之上, 以解决以前的集成方法所存在的问题. 相对于前一种模式也更抽象, 成熟度更高, 复杂度也更高.\n"},{"id":22,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/multiLive/","title":"异地多活 总结","section":"系统架构","content":"\n同城双活 [3] # 【跨机房写，同机房读】\n异地多活 [3] # 一般来说，数据同步的方案有两种：\n一种基于存储系统的主从复制，比如 MySQL 和 Redis。也就是在一个机房部署主库， 在异地机房部署从库，两者同步主从复制, 实现数据的同步。 另一种是基于消息队列的方式。一个机房产生写入请求后，会写一条消息到消息队列， 另一个机房的应用消费这条消息后，再执行业务处理逻辑，写入到存储服务中。 【异步方式同步数据】\n无论是采取哪种方案，数据从一个机房，传输到另一个机房都会有延迟，所以，你需要尽量 保证用户在读取自己的数据时，读取数据主库所在的机房。为了达到这一点，你需要对用户 做分片，让一个用户每次的读写都尽量在同一个机房中。同时，在数据读取和服务调用时， 也要尽量调用本机房的服务。\n【单元化，流量调度】\n总结 [3] # 不同机房的数据传输延迟，是造成多机房部署困难的主要原因，你需要知道，同城多机 房的延迟一般在 1ms~3ms，异地机房的延迟在 50ms 以下，而跨国机房的延迟在200ms 以下。\n同城多机房方案可以允许有跨机房数据写入的发生，但是数据的读取，和服务的调用应该尽量保证在同一个机房中。\n异地多活方案则应该避免跨机房同步的数据写入和读取，而是采取异步的方式，将数据从一个机房同步到另一个机房。\n案例 # 异地多活 阿里 【1】 基于Userid的单元化异地多活 主要改造整个交易链路 交易链路（单元）和非交易链路（中心）之间通过DRC同步数据。单元里的数据是全量、只读的 饿了么 【2】 思路+原则 基于地理位置的异地多活。用户、商家、骑手都会在相同的机房 可用性优先，放宽数据一致性 主要组件 GZS（元数据）+APIRouter（流量路由） SOA Proxy：内部网关、IDC之间调用 Data Replication Center：数据库复制、数据库和cache之间的一致性 Data Access Layer zk,mq在IDC之间的同步 数据一致性 [1][2] # 数据一致性 强一致场景 都读主节点 最终一致性场景 DRC异步同步数据 业务层异步分发数据 数据丢失 通过算法在不同机房都能生成相同的 参考 # 参考 # 《尽在双11:阿里巴巴技术演进与超越》 1.2节\n饿了么异地多活技术实现（一）总体介绍 饿了么框架工具部 知乎专栏\n《28 | 多机房部署：跨地域的分布式系统如何做？》 唐扬\nSET化架构设计 lql_h 未\n看完这篇异地多活的改造，我决定和架构师battle一下｜得物技术 未\n"},{"id":23,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/domain_logic_and_sql/","title":"领域逻辑和SQL","section":"应用架构","content":"\n在企业应用中， 业务逻辑是复杂和庞杂的。 这些业务逻辑应该是被显示， 还是被隐藏， 这是一种选择。 在工作流中， 业务逻辑被从模块中剥离出来， 形成上层的粗粒度的业务流程。 在模块内部， 业务逻辑应该放在内存中， 还是在SQL中， 这也是一种选择。\n拨开项目的DAO层， 你可能会看到大量的SQL字符串， 业务逻辑就隐藏在这里。在有的项目你会看到hibernate之类的ORM框架， PO对象作为一种承载业务逻辑的机制。\n大量复杂的SQL拼接， 在SQL中放入业务逻辑与企业应用架构的分层原则相违背。OO的本质是抽象和分离， 各司其责。 领域逻辑更符合OO的精神。\n领域逻辑中的ORM不仅是一种可重用的对象装载方式， 也是一种虚拟化技术。对象与数据库的映射机制由ORM管控，对象装载和业务逻辑的分离， 表对于对象来说是透明的。就像JVM， 硬件对于Java工程师来说是透明的一样。\n在Domain Logic和SQL之间， 中庸的是Trasaction Script（事务脚本），它根据过程组织业务逻辑，每个过程处理来自表现层的一个单一请求。事务脚本简单的可以看成Domain Logic和Native SQL的结合。\nNative SQL的拥护者会提到SQL的性能优势。在多表查询中，Domain Logic确实没有Native SQL快。 在一条SQL即一个事务情况下，Native SQL会快些。 但在Domain Logic中， framework会使用cache做局部性的优化， 并且cache的对象是能够跨多个事务复用的（hibernate二级缓存）， 缓存策略也是可配置的。Framework并且提供Lazy load机制，在使用时加载对象， 进一步提升性能。\nDomain Logic可以在可理解的代码上做性能的改进，找到那占用了80%时间的20%的代码。 Native SQL可以在高性能的代码上做理解性的改进， 但sql中的逻辑是隐式的， 笔者认为要做到后者不易。 维护占到了软件生命周期的很大一部分， 应该先关注可修改性， 再关注性能。性能的第一原则是“不要提早优化”。\n在长生命周期的企业软件中，需求的迭代和代码的迭代是常态。 改变可能是人们把业务逻辑放在内存中， 使用Domain Logic的主要原因。\n过度复杂的sql拼接让人很难理解， 面条代码， 逻辑不连贯现象容易形成。 在Domain Logic的基础上， DSL（Domain Specific Language）提供了连贯接口， 业务清晰一目了然。 当然在理解程度上， 有人更适应SQL， 有人更适应Domain Logic， 这也是仁者见仁了。\nNative SQL中的sql的重用比Domain Logic中对象的重用更困难。 如果想重用一段SQL，在SQL中嵌入了判断逻辑，SQL的复杂度又就增加了。数据库视图是表的接口，可以定义一个视图， query重用定义好的视图。但视图有局限性， 只有select操作， 没有update操作， 如果有DML要求， 还需要定义存储过程。\n使用视图和存储过程提供的封装是不完全的。 在企业应用中，数据会来源于多个数据源， 多个数据库， xml文件，nosql数据库， 遗留系统等。 在这个情况中，数据存取的完全封装确实只能在应用的分层中实现。\nDomain Logic提供了抽象层次和模块化的机制， 对象装载和实际业务的分离， 好的对象装载机制会零入侵业务逻辑，如果Annotation用的是JSR标准， 把hibernate替换成OpenJPA也比较容易。\nDomain Logic要求对framework有好的把握， 一定的驾驭能力， 问题的解决能力。 Domain Logic里的对象有更多的约束条件，更多的模式， 比如一对一，多对一，多对多。 SQL相对更容易掌握， 代码直接可控。\n如果想要有可移植性，请不要使用sql。 各个语言都有自己的方言， 语法有略微的不同。Id的增长方式不同，有sequence， 有自增的， 有全局的。Mysql有limit关键字，oracle有rowid和rownum, db2有ROW_NUMBER() over(). 如果有数据库移植的需求， 已经写的Native SQL就会有大的改动。\n参考： # Domain Logic and SQL\n"},{"id":24,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/SRE/SRE/sreWorkbook/","title":"《SRE 工作手册》","section":"SRE","content":"\n参考 # SRE 实践的知识体系梳理\n"},{"id":25,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/SRE/SRE/sreWorkbookBasic/","title":"SRE 五大根基","section":"SRE","content":"\n目录 # SRE五大根基 # 实践SLO # 监控 # 告警 # 减少琐事 # 简单化 # 参考 # 《Google SRE工作手册》第二期SRE五大根基之一：SLO V *** 《Google SRE工作手册》第二期SRE五大根基之二：监控 V *** {% post_link \u0026lsquo;sreWorkbook\u0026rsquo; %} self {% post_link \u0026lsquo;sreWorkbookBasicSLO\u0026rsquo; %} "},{"id":26,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/SRE/SRE/sreWorkbookBasicSLO/","title":"SRE 五大根基-SLO","section":"SRE","content":"\n目录 # SRE 五大根基 之 SLO[1] # 步骤1. 制定SLO # 服务的SLO # VALET[Home Depot] Volume Avail Latency Errors Ticket 数据服务的SLO # Go to Page {% post_link \u0026lsquo;kafkaSLO\u0026rsquo; %} SLI # 服务类型 SLI类型 请求驱动 可用性，延迟，质量 流水线 时效性，正确率，覆盖率 存储 持久性 步骤2. 获得干系人认同 # SLO 仪表板[趋势] # 步骤3. 持续监控 改进SLO # 变更SLO 变更SLI实现 着手于现实的SLO 迭代 步骤4. 错误预算 SLO决策 # 基于SLO和错误预算的决策 # 步骤5. 进阶 # 用户旅程建模 依赖关系建模 参考 # 《Google SRE工作手册》第二期SRE五大根基之一：SLO V *** 《Google SRE工作手册》 第二章 "},{"id":27,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/SRE/SRE/sreWorkbookBasicAlert/","title":"SRE 五大根基-报警","section":"SRE","content":"\n告警设定考量 # 精准率\n减少误告警\n查全率\n减少漏告警\n检测用时\n过长 影响错误预算 重置用时\n过长 增长内存和IO开销 告警设定方法 # 基础 方法1 目标错误率 \u0026gt;= SLO阈值 window 方法2 延长报警时间窗口 方法3 延长告警触发前的持续时间 燃烧率 方法4 根据燃烧率发出告警 方法5 基于多个燃烧率的告警 方法6 基于多个窗口 多个燃烧率的告警 参考 # 《Google SRE工作手册》第四期基于SLO的告警配置及实践分享 V 《Google SRE工作手册》 第5章 "},{"id":28,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/%E5%AE%B9%E9%87%8F%E4%BF%9D%E9%9A%9C/capacityGuarantee/","title":"容量保障与全链路压测","section":"容量保障","content":"\n容量保障Overview[2] # 大促容量保障的三项重点工作： 大促流量预估 大促容量测试 大促容量保障预案 容量预测 # 容量预测[1] # 我首先给出了**“皮尔逊相关系数”**这个工具，对服务 TPS 和 CPU 利用率之间的相关度进 行了定量分析，根据相关度的强弱，分别采取不同策略。其中，重点讲到了在两者弱相关 时的应对策略，如果能够穷举出尽可能多的相关特征，可以通过特征选取的方式对服务进 行画像，提升预测准确率；如果特征非常难找，那么可以依靠概率表的方式曲线救国。\n随着服务不断迭代，容量也在不断变化，我与你分析的第二个问题，就是如何平衡好服务 迭代和容量预测频率的关系。根据服务发布窗口（或其他变更时间点）建立滑动窗口机 制，既保证了在服务变更后能够尽快地更新模型，又不至于带来大量的计算量，是一个不 错的实践方式。\n业务场景变化也会导致容量变化，针对这个问题，我结合之前提到的全链路压测工作，通 过建立全链路压测和容量预测双向校准的机制，提前对变化的业务场景进行预测，识别容 量风险。\n全链路压测 # 核心功能[4] # 压测-部署架构 # 施压机的分布[3]\n大部分仍然是跟线上系统在同机房内，少量会在公有云节点上 以将全球（主要是国内）的 CDN 节点作为施压机 更加真实地模拟真实用户从全球节点进入的真实访问流量 成本过高，技术条件和细节难 压测的读写流量[3]\n读流量 写流量 对压测的写请求做专门的标记。 当请求要写数据库时，由分布式数据库的中间件框架中的逻辑来判断这个请求是否是压测请求，如果是压测写请求则路由到对应的影子库中，而不是直接写到线上正式的库中。 改造 全链路压测[5] # 数据隔离 物理隔离 vs. 逻辑隔离 [见表1] 中间件改造 eg. MQ改造 ​ Producer: 判断请求带压测标识，转换到数据体（msg）中 ​ Consumer: 判断数据（msg）中有压测标识， 恢复压测标识至请求中 应用服务改造 绕开限制逻辑 数据隔离前置 Mock 逻辑 ​ 表1 逻辑隔离 vs. 物理隔离\n隔离类型 逻辑隔离 物理隔离 中间件改造 小，几乎不需要改造 大，需要保证压测流量标识能一路透传不丢失 业务侵入性 大，会影响表结构设计 小， 对数据实体没有侵入 数据清洗难度 大， 需根据每个数据实体的标识单独定制清洗规则 小，压测数据都在影子表 可扩展性 弱， 新数据实体均需要设计新的压测标识 强， 流量标识为统一形式， 且与数据无关 安全性 弱， 与真实数据写入同一张表， 一旦隔离逻辑有疏漏， 会影响真实用户 强， 与真实数据分开存储， 即便考虑不周 ，也不会影响真实数据 参考 # 《09 | 容量预测（下）：为不同服务“画像”，提升容量预测准确性》 吴骏龙 《 13 | 大促容量保障体系建设：怎样做好大促活动的容量保障工作（下）》 吴骏龙 稳定性实践：容量规划之压测系统建设 极客时间？ 全链路压测体系建设方案的思考与实践 阿里 *** 《05 | 全链路压测：系统整体容量保障的“核武器”（上）》 吴骏龙 "},{"id":29,"href":"/www6vDistributed/docs/%E4%B8%80%E8%87%B4%E6%80%A7/raft/","title":"Raft协议","section":"一致性","content":"\n总结 # 从本质上说，Raft 算法是通过一切以领导者为准的方式，实现一系列值的共识和各节点日志的一致.\nRaft： leader + term + peers\nRaft-分区脑裂（成员变更的问题） # 分区脑裂[非majority有uncommited log、 term1]\n分区脑裂[majority可以同步log、 term2]\nRaft-领导者选举 # Raft-复制日志 # 副本数据是以日志的形式存在的，其中日志项中的指令表示用户指定的数据。 Raft 是通过以领导者的日志为准，来实现日志的一致的。 在 Raft 中日志必须是连续的 日志完整性最高的节点才能当选领导者 参考 # raft 动画 good The Raft Consensus Algorithm good 动画 各种系统实现 未 Raft Distributed Consensus Algorithm Visualization 动画 未 Raft对比ZAB协议 raft协议和zab协议有啥区别？ 一张图看懂Raft 未 Raft 为什么是更易理解的分布式一致性算法 未 分布式协议与算法实战 - 07 | Raft算法（一）：如何选举领导者？ 韩健 *** 分布式协议与算法实战 - 08丨Raft算法（二）：如何复制日志？.pdf 韩健 *** Raft 分布式系统一致性协议探讨 腾讯 未 论文 Raft一致性算法论文译文 In Search of an Understandable Consensus Algorithm(Extended Version) raft "},{"id":30,"href":"/www6vDistributed/docs/%E4%B8%80%E8%87%B4%E6%80%A7/splitBrain/","title":"Split Brain","section":"一致性","content":"\n关键词: 脑裂, fence, Quorums , epoch\n脑裂： 类似 CAP中的P Partition tolerance(分区容错性): 网络分区发生时，一致性和可用性两难全\n一. 通用解决方案 # Quorums Redundant communications，冗余通信的方式 Fencing 二. 系统 # / 现象 解决方案 kafka kafka脑裂现象:1. 存在多个controller 2. consumer的splitBrain controller使用epoch来避免脑裂 elastic search 配置discovery.zen.minimum_master_nodes，类似Quorums zookeeper 两个leader[3] Quorums leader单调递增的epoch raft脑裂 两个majority [2] Quorums + term redis脑裂、mysql脑裂\n参考: # redis 脑裂等极端情况分析 Redis Cluster is not able to guarantee strong consistency. / In general Redis + Sentinel as a whole are a an eventually consistent system\nraft协议 self 脑裂是什么？Zookeeper是如何解决的？ ​\n​ ​\n"},{"id":31,"href":"/www6vDistributed/docs/%E4%B8%80%E8%87%B4%E6%80%A7/consistent/","title":"分布式一致性 总结","section":"一致性","content":"\n分布式一致性 # [粉色-Unavailable] 在某些网络故障情况下不可用。为了确保安全，一些或所有节点必须暂停操作。\n[黄色-Sticky Available] 只要客户端只与相同的服务器通信而不切换到新的服务器，就可在每个非故障节点上使用。\n[蓝色-Total Available] 即使网络完全瘫痪，也可在每个非故障节点上使用。\n一致性 # 强一致性模型 # 强一致性 协议 特性 工程 线性一致性[chat] 2PC\n3PC #1 延迟大，吞吐低。全局锁资源 JTA(XA)\n{% post_link \u0026rsquo;transactionSeata\u0026rsquo; Seata XA,AT 非入侵 %} self 顺序一致性[chat] Paxos #1 难理解，延迟大，吞吐中等，全局锁资源 Google Chubby 顺序一致性 {% post_link \u0026lsquo;zookeeperZab\u0026rsquo; Zab %} self\n逻辑时钟 类似多线程程序执行顺序的模型 Zookeeper的读 1.两个主流程，三个阶段 2. Quorum:2f+1个节点，允许f个节点失败 强一致性 {% post_link \u0026lsquo;raft\u0026rsquo; %} self 相对Paxos简单。主从，三个阶段 Go to Page self 逻辑时钟\nLamport提出逻辑时钟是为了解决分布式系统中的时序问题，即如何定义a在b之前发生. Java中有happen-before 图2. 逻辑时钟 logic-clock\n线性一致性 Linearizability\n线性一致性 #1： 严格一致性（Strict Consistency）或者原子一致性（Atomic Consistency） 一个操作对于系统的其他部分是不可中断的\n顺序一致性 Sequential\n任何一次读写操作都是按照某种特定的顺序。 所有进程看到的读写操作顺序都保持一致。\n顺序一致性虽然通过逻辑时钟保证所有进程保持一致的读写操作顺序，但这些读写操作的顺序跟实际上发生的顺序并不一定一致。而线性一致性是严格保证跟实际发生的顺序一致的。 Paxos、ZAB 和 RAFT 有以下几个主要的共同点[Claude]:\n都通过选举 Leader 来接受客户端请求, Leader 接收写操作,然后同步给 Follower 节点,保持集群数据的一致性。 都使用**日志(log)**来记录节点状态的变更,Follower 节点通过应用相同的日志来保持数据一致。 都通过多个阶段来实现一致性,例如Prepare 阶段 和 Commit 阶段。 都需要超过半数以上的节点达成一致(quorum),才能提交日志。 弱一致性模型 # 因果一致性 # 因果一致性 协议 特性 工程 因果一致性 向量时钟 Vector clock[向量时钟] 图1 微信朋友圈的评论, Dynamo 向量时钟 图1. 向量时钟 vector-clock\n客户端为中心的一致性（Client-centric Consistency） # 客户端为中心的一致性\n最终一致性 以客户端为中心的一致性为单一客户端提供一致性保证，保证该客户端对数据存储的访问的一致性，但是它不为不同客户端的并发访问提供任何一致性保证. 类型\n单调读一致性（Monotonic-read Consistency）\n**kafka的消费者的单调读 ** 单调写一致性（Monotonic-write Consistency） 读写一致性（Read-your-writes Consistency） 写读一致性（Writes-follow-reads Consistency） 最终一致性 # 最终一致性 协议 特性 工程 反熵Anti-Entropy\nGossip Cassandra， redis的集群状态的同步机制 Sloppy quorum # Sloppy quorum 特性 工程 R+W\u0026gt;N[ReadQurum-WriteQurum] 可定制 Dynamo, Cassandra 定制灵活 最终一致性-工程 # TCC # TCC 流程 1.主流程控制整个事务\n2.分流程提供Confirm和Cancel方法。 阶段 Try: 阶段1的业务执行\nConfirm: 阶段2的业务执行\nCancel: 回滚Try阶段执行的业务流程和数据 TCC FMT\n{% post_link \u0026rsquo;transactionSeata\u0026rsquo; Seata TCC %} self 基于事务消息的分布式事务 # EBay模式 [8]\n正向流程\n[本地事务+幂等业务接口+half消息] 消息状态\n初始化：消息为待处理状态\n业务成功：消息为待发送状态\n业务失败：消息删除 反向流程（异常流程，补偿流程）\n中间件询问业务执行结果，更新消息状态 工程 {% post_link \u0026lsquo;mqRocketmqTransaction\u0026rsquo; RocketMQ事务消息 %} self\n基于本地消息的分布式事务 # Saga流程 # Saga 1PC (一阶段) 基于补偿的消息驱动的用于解决long-running process业务。 工程\n{% post_link \u0026rsquo;transactionSeata\u0026rsquo; Seata Saga %} self 弱一致性-工程 # 补偿 # 流程\n状态查询（成功or失败）+补偿 流程细节\n定时校验异常 + 补偿 State Machine \u0026amp;\u0026amp; Primary-copy # state machine replication \u0026amp;\u0026amp; primary-copy\n复制状态机(state machine replication) 多个节点上，从相同的初始状态开始，执行相同的一串命令，产生相同的最终状态\n状态机 + 命令 -\u0026gt; 重放\nstate machine replication例子\nmysql主从复制 slave relay log, 基于sql语句的复制[9];\nredis AOF\nprimary-copy例子:\nzookeeper的主从复制;\nmysql主从复制 slave relay log, 基于行的复制[9];\nredis RDB 快照;\n参考 # 一致性 # 保证分布式系统数据一致性的6种方案 高可用架构 *** 深入解析NoSQL数据库的分布式算法 *** ZooKeeper真不是最终一致性的，而是顺序一致性 陈东明 为什么程序员需要关心顺序一致性（Sequential Consistency）而不是Cache一致性（Cache Coherence） carlosstephen 分布式系统：一致性模型 阿里 Overview *** ENode 1.0 - Saga的思想与实现 汤雪华 《大数据日知录：架构与算法》 张俊林 Base: An Acid Alternative Ebay模式 *** 如何选择分布式事务解决方案？ ali *** {% post_link \u0026lsquo;NoSQL\u0026rsquo; %} self 一致性 （建议收藏）万字长文总结分布式事务，总有一款适合你 *** 腾讯 《数据密集型应用系统设计》笔记五：第五章 数据复制 未 应用 # 数据一致性检测应用场景与最佳实践 阿里 未 向量时钟 # 向量时钟Vector Clock in Riak Why Vector Clocks Are Hard 未 Dynamo: Amazon’s Highly Available Key-value Store paper 未\n向量时钟的变种 版本向量（Version vector） 版本控制机制 分布式系统：向量时钟 阿里 肖汉松 *** "}]