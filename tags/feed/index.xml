<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Feed on Distributed</title>
    <link>https://www6v.github.io/www6vDistributed/tags/feed/</link>
    <description>Recent content in Feed on Distributed</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Wed, 02 Mar 2022 19:19:10 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vDistributed/tags/feed/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Feed流 总结 *</title>
      <link>https://www6v.github.io/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/feed/</link>
      <pubDate>Fri, 13 Sep 2019 19:35:02 +0000</pubDate>
      <guid>https://www6v.github.io/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/feed/</guid>
      <description>&#xA;Feed总结 # 消息同步模型 # 消息同步模型- 左:BCDEF的发件箱，右:A的收件箱&#xA;基于Timeline的消息库设计 # 基于Timeline的消息库设计 - 上：用于写扩散消息同步，下：全量历史消息，读扩散消息同步&#xA;推拉结合 # 基于用户类型的Timeline推拉结合(读扩散/写扩散混合) - 上面是发布流程，下面是阅读流程&#xA;读扩散 vs 写扩散 # 拉模式(读扩散) 推模式(写扩散)[推荐使用] 发布 个人页Timeline（发件箱） 粉丝的关注页（收件箱） 阅读 所有关注者的个人页Timeline 自己的关注页Timeline 网络最大开销 用户刷新时 发布Feed时 读写放大 放大读：读写比例到1万:1 放大写减少读：读写比例到50:50 优点 只要写一次 接收端消息同步逻辑会非常简单 缺点、副作用 1.读被大大的放大&#xA;2.响应时间长 消息写入会被放大， 数据会极大膨胀， 针对副作用的优化-推拉结合 1.大V采用拉模式，普通用户使用推模式&#xA;2.对活跃粉丝采用推模式，非活跃粉丝采用拉模式 场景 # 场景 Timeline IM单聊 三个Timeline IM群聊 1 + N个Timeline 朋友圈 1 + N个Timeline 微博 大V发一条微博就是 1 + M个Timeline（M &amp;laquo; N，N是粉丝数） Rank # 参考 # feed流拉取，读扩散，究竟是啥？ 如何打造千万级Feed流系统 TableStore Timeline：轻松构建千万级IM和Feed流系统 现代IM系统中消息推送和存储架构的实现 Feed流系统设计-总纲 未 </description>
    </item>
    <item>
      <title>系统设计 总结</title>
      <link>https://www6v.github.io/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/systemDesign/</link>
      <pubDate>Wed, 02 Mar 2022 19:19:10 +0000</pubDate>
      <guid>https://www6v.github.io/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/systemDesign/</guid>
      <description>特点 设计 Feed流 读写比例100:1&#xA;消息必达性要求高 非稳定的账号关系 {% post_link &amp;lsquo;feed&amp;rsquo; %} 秒杀系统 {% post_link &amp;lsquo;secKillSummary&amp;rsquo; %} {% post_link &amp;lsquo;seckill&amp;rsquo; %} 计数系统[2] 数据量巨大&#xA;访问量大，性能要求高&#xA;对于可用性、数字的准确性要求高 + 方案1 数据库 + 缓存 -&amp;gt; 按照 weibo_id做分库分表。数据库和缓存之间无法保证数据的一致性 [2]&#xA;+方案2 全部写入redis -&amp;gt; MQ异步写，批量合并写。redis昂贵， 存储优化，冷热分离 [2] 抢红包系统[3] 交易类信息:红包发、抢、拆、详情列表 展示类信息: 收红包列表、发红包列表 + 架构设计，理论基础是快慢分离。红包入账是一个分布事务，属于慢接口。而拆红包凭证落地则速度快 [3]&#xA;+ 高并发 set化， 局部化-控制同一红包并发个数 排行榜系统 参考 # 计数系统 # 计数系统架构实践一次搞定 | 架构师之路 未&#xA;&amp;laquo;37丨计数系统设计（一）：面对海量数据的计数器要如何做？&amp;raquo; 唐扬&#xA;红包系统 # 揭秘微信红包架构、抢红包算法和高并发和降级方案 微信红包&#xA;揭秘微信红包：架构、抢红包算法、高并发和降级方案&#xA;微信红包后台系统设计 未&#xA;微信高并发资金交易系统设计方案&amp;mdash;&amp;ndash;百亿红包背后的技术支撑&#xA;SET化、请求排队串行化、双维度分库表 未</description>
    </item>
  </channel>
</rss>
