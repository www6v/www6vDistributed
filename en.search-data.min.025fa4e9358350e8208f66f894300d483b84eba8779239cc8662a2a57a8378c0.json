[{"id":0,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/cleanCode/","title":"整洁架构 *","section":"应用架构","content":"\n整洁架构 Clean Architecture # 核心观点 [7][8]\n不与框架绑定\njava-spring, Quarkus 可测试\nmock- gomock, Testify 不与UI绑定 不与数据库绑定\nDDD 中的Repo 不依赖任何外部代理 Go的实现 [9][10]\n参考 # 《24 直播：框架之上的业务分层》 体系课_Go高级工程师实战营(完结)\nThe Clean Architecture\nclean-architecture-go-v2 git\ngo-clean-arch\nGolang 简洁架构实战 未\nGo整洁架构实践 未\n"},{"id":1,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/SRE/SRE/sre/","title":"SRE 总结","section":"SRE","content":"\nSRE [1] # SRE = PE（Production Engineer） + 工具平台开发 + 稳定性平台开发 工具平台团队，负责效能工具的研发，比如实现 CMDB、运维自动化、持续交付流水线以 及部分技术运营报表的实现，为基础运维和应用运维提供效率平台支持。 稳定性平台团队，负责稳定性保障相关的标准和平台，比如监控、服务治理相关的限流降 级、全链路跟踪、容量压测和规划。 组织架构图 [1] # 故障复盘 [2] # 黄金三问 第一问：故障原因有哪些？ 第二问：我们做什么，怎么做才能确保下次不会再出现类似故障？ 第三问：当时如果我们做了什么，可以用更短的时间恢复业务？ 故障判定的三原则 健壮性原则。 第三方默认无责。 分段判定原则。 5W 分析法 Google SRE Principle [5] # 运营是软件问题 服务水平目标SLO 减少琐事 用自动化的方式减少琐事 自动化 ** 统一环境， IaC CaC， ** IaC: Terraform, Ansible, Pulumi 生产环境中进行测试 统一 版本管理，制品库，cmdb 可观测性 降低失败成本 复盘 从失败中学习 共享所有权 个人安全，责任共担 拥抱风险 + 错误预算 Google SRE 实践总结 [5] # 确保长期关注研发工作 在保障SLO的前提下最大化迭代速度 监控系统 + insight，根因 应急事件处理 变更管理 ITIL 需求预测和容量规划 + AIOps 资源部署 效率与性能 参考 # 《09｜案例：互联网典型的SRE组织架构是怎样的？》 赵成 《08｜故障复盘：黄金三问与判定三原则》 赵成 xxx SRE 的工作介绍 SRE大佬 未 SRE核心概念与可观测性介绍 中国DevOps社区 刘峰 +《SRE google 运维解密》\n《SRE google 运维解密》读书笔记 （一） 未 《SRE google 运维解密》读书笔记 （二） 未 《SRE google 运维解密》读书笔记 （三） 《SRE google 运维解密》读书笔记 （四） "},{"id":2,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/distributedSystemPattern/","title":"分布式系统 模式 *","section":"系统架构","content":"\nFault Tolerant Consensus # 英文 中文 Paxos Paxos Quorum *** Quorum Generation Clock *** 世代时钟（Generation Clock） Pattern Sequence for implementing replicated log # 英文 中文 Example \u0026amp; 扩展 Replicated Log *** 复制日志（Replicated Log） Raft Write-Ahead Log *** 预写日志（Write-Ahead Log） MySQL redo log\nHBase WAL\nRedis aof Low-Water Mark *** 低水位标记（Low-Water Mark） High-Water Mark *** 高水位标记（High-Water Mark） kafka HW {% post_link \u0026lsquo;streamingFlinkWatermarkWindow\u0026rsquo; %} HeartBeat *** 心跳（HeartBeat） {% post_link \u0026lsquo;crashDetect\u0026rsquo; %} Leader and Followers ***\nleader election 领导者和追随者（Leader and Followers） Go to Page Go to Page {% post_link \u0026lsquo;zookeeperZab\u0026rsquo; %}\nes 选主[todo] Follower Reads *** 追随者读取（Follower Reads） Segmented Log *** 分段日志（Segmented Log） Kafka segment Request Pipeline 请求管道（Request Pipeline） Singular Update Queue 单一更新队列（Singular Update Queue） Single Socket Channel 单一 Socket 通道（Single Socket Channel） Atomic Commit # 英文 中文 Example \u0026amp; 扩展 Two Phase Commit 两阶段提交（Two Phase Commit） 两阶段变种 Fixed Partitions / Key-Range Partitions / Kubernetes or Kafka Control Plane # 英文 中文 Example \u0026amp; 扩展 Lease *** 租约（Lease） {% post_link \u0026lsquo;crashDetect\u0026rsquo; %} ETCD Lease K8s Lease\nEureka Lease State Watch *** 状态监控（State Watch） Idempotent Receiver 幂等接收者（Idempotent Receiver） Logical Timestamp usage # 英文 中文 Example \u0026amp; 扩展 Lamport Clock Lamport 时钟（Lamport Clock） Versioned Value *** 有版本的值（Versioned Values） tikv mvcc[TSO]\nCockroachDB mvcc[HLC]\nHybrid Clock *** 混合时钟（Hybrid Clock） CockroachDB Gossip Dissemination *** Gossip 传播（Gossip Dissemination） Redis Gossip Consistent Core 一致性内核（Consistent Core） Version Vector 版本向量（Version Vector） Others # Request Batch Clock-Bound Wait Emergent Leader Request Waiting List Self # Example \u0026amp; 扩展 time wheel {% post_link \u0026rsquo;timedTask\u0026rsquo; %} 参考 # Patterns of Distributed Systems 《分布式系统模式》中文版 Go to Page self "},{"id":3,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/api_design/","title":"OpenAPI 设计","section":"应用架构","content":"\nREST API 设计 规范 # OpenAPI Specification 业界标准\nA Visual Guide to What\u0026rsquo;s New in Swagger 3.0 OpenAPI Specification - Version 3.0.2 Google API Design Guide\n谷歌API设计指南 API 设计模式 # RPC ROA(Rest-Oriented Architecture) 通常RESTful风格对API设计者的要求是比较高的，主要的难点在于面向资源设计要求开发者事先做好规划，将后端数据模型与API服务模型相匹配。\n面向资源设计API # 资源模型 资源分类管理 资源关系 ECS TAG功能详解 资源组\n服务容错处理 # 同步请求的Timeout[2] 异步请求方式 错误码\nTagResources 错误码 参考: # 云服务OpenAPI的7大挑战，架构师如何应对？ 阿里技术 虚明 超时和重试总结 self "},{"id":4,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/feed/","title":"Feed流 总结","section":"系统设计","content":"\nFeed总结 # 消息同步模型 # 消息同步模型- 左:BCDEF的发件箱，右:A的收件箱\n基于Timeline的消息库设计 # 基于Timeline的消息库设计 - 上：用于写扩散消息同步，下：全量历史消息，读扩散消息同步\n推拉结合 # 基于用户类型的Timeline推拉结合(读扩散/写扩散混合) - 上面是发布流程，下面是阅读流程\n读扩散 vs 写扩散 # 拉模式(读扩散) 推模式(写扩散)[推荐使用] 发布 个人页Timeline（发件箱） 粉丝的关注页（收件箱） 阅读 所有关注者的个人页Timeline 自己的关注页Timeline 网络最大开销 用户刷新时 发布Feed时 读写放大 放大读：读写比例到1万:1 放大写减少读：读写比例到50:50 优点 只要写一次 接收端消息同步逻辑会非常简单 缺点、副作用 1.读被大大的放大\n2.响应时间长 消息写入会被放大， 数据会极大膨胀， 针对副作用的优化-推拉结合 1.大V采用拉模式，普通用户使用推模式\n2.对活跃粉丝采用推模式，非活跃粉丝采用拉模式 场景 # 场景 Timeline IM单聊 三个Timeline IM群聊 1 + N个Timeline 朋友圈 1 + N个Timeline 微博 大V发一条微博就是 1 + M个Timeline（M \u0026laquo; N，N是粉丝数） Rank # 参考 # feed流拉取，读扩散，究竟是啥？ 如何打造千万级Feed流系统 TableStore Timeline：轻松构建千万级IM和Feed流系统 现代IM系统中消息推送和存储架构的实现 Feed流系统设计-总纲 未 "},{"id":5,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/middleStage/","title":"中台战略","section":"系统架构","content":"\n目录 # 中台全景图 # 中台全景图[6] 中台和微服务[4] 中台\n业务中台 核心业务层 技术中台 iaas+paas 数据中台 理念 阿里提出： 大中台， 小前台\n业务中台 # 业务中台 # 阿里共享服务 [6] 淘宝 天猫 共享 商品，交易，店铺等服务 京东业务中台 [5] 技术中台 [1][2][3] # 参考 # 《企业IT架构转型之道-阿里巴巴中台战略思想与架构实战》 钟华 全面异步化：淘宝反应式架构升级探索 淘宝应用柔性架构的探索 《微服务架构核心20讲-如何理解阿里巴巴提出的微服务中台战略？》 杨波 中小型电商相当适配：京东商城系统架构设计原则精炼 "},{"id":6,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/SRE/%E6%95%85%E9%9A%9C%E6%A8%A1%E5%9E%8B/faultModel1/","title":"故障模型-应用层","section":"故障模型","content":"\n故障模型-Application \u0026amp; Data # OOM # + 堆内 【10】\r- PermGen\r+ 原因：反射类多\r+ 解决\r先jmap，后btrace【11、12 case2】\r- heap\r解决：对比Fullgc后的相同对象的数量、大小\r+ 堆外Native\r参考 Go to Page self 如何排查Java内存泄露(内附各种排查工具介绍) 不闻 生产环境下持久带满导致FullGC，如何跟踪 Go to Page self 应用性能变差 # + 原因\r- 锁 - heap\r+ fullgc后没有空间\r原因：内存泄露\r工具：heap dump\r+ fullgc后有空间\r解决：设置门槛，过滤大量短生命周期对象【12 case1，13】\r- gc停顿长\r解决：【12 case3，13】\r参考 Go to Page self Go to Page self 听阿里巴巴JVM工程师为你分析常见Java故障案例 *** Load过高 # + cpu load高【13】\r- 启动阶段\r原因：JIT编译器\r解决：分层编译\r- 运行阶段\r原因：有热点方法\r工具：MAT\r- 解决\r工具【8、9】\r数据收集 4、5\r参考\n不正当使用HashMap导致cpu 100%的问题追究 王宏江\n一个由正则表达式引发的血案 CPU飚高 vjtools useful-scripts 听阿里巴巴JVM工程师为你分析常见Java故障案例 *** 进程Hang # 进程被杀， JVM crash # + 原因：\rJNI Unsafe\r+ 工具：core dump\r异常 # + 启动异常\r+ 心跳异常\rCI/CD # + 环境错误\r+ 部署包错误\r+ 配置错误，误删\r系统单点 # + 原因: 设计问题\r+ 解决：服务去状态，多实例部署\r异步阻塞同步 # 依赖超时，依赖异常 # + 解决【15,16】\r- 区分强弱依赖\r- 压测 + 容量规划\r- 熔断 \u0026amp;\u0026amp; 降级\r参考 Go to Page self {% post_link \u0026lsquo;stability\u0026rsquo; %} self 业务线程池满 # + 最佳实践:【4】\r+ 案例：【14】\r- 问题：使用blockingQueue.put\r- 解决：blockingQueue.offer（time超时机制）+限制队列长度\r- 最佳实践：生产上线程池的core Size和poolSize设置的一样，请求不在队列里排队\r参考 Go to Page self 从一个故障说说Java的三个BlockingQueue 阿里毕玄 流控不合理 # 其他-数据收集 # + 1.Heap dump\r+ 2. GC 日志\r+ 3. core dump\r+ 4. 线程stack\r+ 5.os进程信息\r参考 # 大纲 # 超全总结 | 阿里如何应对电商故障？神秘演练细节曝光 阿里巴巴 周洋 Application \u0026amp; Data # 如何检测 Web 服务请求丢失问题 Nginx tracing + Tomcat tracing 系统中的故障场景建模 应用层 大方法 codecache "},{"id":7,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/designPrinciple/","title":"设计原则","section":"设计原则","content":"\nLaw 定律 # 奥卡姆剃刀原理 *** # 如果对于一个现象有好几种解释, 那么最简单的解释往往是最正确的.\n排队理论 # Little\u0026rsquo;s 定律 -\u0026gt; 应用 ： 线程池中多线程个数的确定。\n康威定律 # organizations which design systems \u0026hellip; are constrained to produce designs which are copies of the communication structures of these organizations\n衍生: 1).DDD context 2).微服务模块划分 Amdahl定律, 通用扩展定律(Universal Scalability Law, USL) # CAP/BASE # 复杂度 简化本质复杂度，消除偶发复杂性. # 有三个问题可能会产生偶发复杂度。\n第一个：由于日程或其他外部压力而导致临时大量削减代码。 第二个是复制。 第三个诱因是不可逆性，您做出的无法逆转的所有决定都将最终导致某种程度的偶发复杂度。 架构师： 去熵， 去复杂度。\n原则 Principle # SOLID *** # 开闭原则 [3] 对于扩展是开放的（Open for extension） 对于修改是关闭的（Closed for modification） Happy path \u0026amp; Sad path 代码执行路径： happy path 和 sad path分离。 # 测试用例： happy path用例。 sad path用例， 使用@Exception（Junit4）， fail（JUnit3）。\n笛米特法则 # 只和最亲密的朋友讲话(talk only to your immediate friends). 任何对象都不需要知道与之交互的对象的任何细节.\nMongoDB设计哲学 # Databases are specializing – the “one size fits all” approach no longer applies.\n“KISS”原则 - Keep it simple and stupid # 衍生: Rob Pike - Simplicity is Complicated\nRule of least power（够用就好）的原则。 # 这个原则是由 WWW 发明者 Tim Berners-Lee 提出的，它被广泛用于指导各种 W3C 标准制定\n参考 # 对开发人员有用的定律、理论、原则和模式 *** 滴滴杜欢：大型微服务框架设计实践 {% post_link \u0026lsquo;designOCPspi\u0026rsquo; %} self "},{"id":8,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/%E7%A8%B3%E5%AE%9A%E6%80%A7/stability/","title":"稳定性总结","section":"稳定性","content":"\n关键词: 容量规划, 压测, 强弱依赖, 关键词: 故障模型, 故障演练, 故障注入\n稳定性总结 # 参考： # 超全总结 | 阿里如何应对电商故障？神秘演练细节曝光 阿里巴巴-周洋（花名中亭） 故障注入， 故障演练 稳定性思考-强弱依赖 阿里中间件团队博客 稳定性思考-强弱依赖2 阿里中间件团队博客 中间件技术及双十一实践·稳定性平台篇 阿里中间件（Aliware） 强弱依赖， 容量规划 \u0026laquo;尽在双11阿里巴巴技术演进与超越\u0026raquo; "},{"id":9,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/designOCPspi/","title":"开闭原则 - SPI","section":"设计原则","content":"\n开闭原则（Open Closed Principle） # open for extension, but closed for modification\n开闭原则实现 - SPI # SPI Java SPI Dubbo SPI ExtensionLoader Spring SPI @FunctionalInterface @Order(Ordered.LOWEST_PRECEDENCE) public interface MyBeanPostProcessor extends BeanPostProcessor { // define your methods here } 参考 # Java SPI机制以及和Dubbo/Spring SPI对比 面试官问烂的Dubbo中SPI机制的源码解析 *** 未\n源码级深度理解 Java SPI 未\n剖析 SPI 在 Spring 中的应用 未\n"},{"id":10,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/systemDesign/","title":"系统设计 总结","section":"系统设计","content":"\n特点 设计 Feed流 读写比例100:1\n消息必达性要求高 非稳定的账号关系 {% post_link \u0026lsquo;feed\u0026rsquo; %} 秒杀系统 {% post_link \u0026lsquo;secKillSummary\u0026rsquo; %} {% post_link \u0026lsquo;seckill\u0026rsquo; %} 计数系统[2] 数据量巨大\n访问量大，性能要求高\n对于可用性、数字的准确性要求高 + 方案1 数据库 + 缓存 -\u0026gt; 按照 weibo_id做分库分表。数据库和缓存之间无法保证数据的一致性 [2]\n+方案2 全部写入redis -\u0026gt; MQ异步写，批量合并写。redis昂贵， 存储优化，冷热分离 [2] 抢红包系统[3] 交易类信息:红包发、抢、拆、详情列表 展示类信息: 收红包列表、发红包列表 + 架构设计，理论基础是快慢分离。红包入账是一个分布事务，属于慢接口。而拆红包凭证落地则速度快 [3]\n+ 高并发 set化， 局部化-控制同一红包并发个数 排行榜系统 参考 # 计数系统 # 计数系统架构实践一次搞定 | 架构师之路 未\n\u0026laquo;37丨计数系统设计（一）：面对海量数据的计数器要如何做？\u0026raquo; 唐扬\n红包系统 # 揭秘微信红包架构、抢红包算法和高并发和降级方案 微信红包\n揭秘微信红包：架构、抢红包算法、高并发和降级方案\n微信红包后台系统设计 未\n微信高并发资金交易系统设计方案\u0026mdash;\u0026ndash;百亿红包背后的技术支撑\nSET化、请求排队串行化、双维度分库表 未\n"},{"id":11,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/disaggregationOfComputeAndStorage/","title":"存算分离-数据应用","section":"系统架构","content":"\n存算分离 存算一体 RMDB MySQL Cluster MySQL Group Replicatoin(MGR) NewSQL,HTAP TiDB[tidb,tikv] , openGaussDB[2], CockroachDB? Aurora[10], PolarDB[3], PGXC风格[1] 大数据 clickhouse, hbase ES MQ Pulsar kafka, rocketmq 文件系统 Ceph[PG, ODS], PolarFS[5] KV GaussDB(for Redis) [4], Codis redis Cluster 其他 serverless[FasS, BaaS] PolarDB [7] 基于Redo Log物理复制实现的一写多读共享存储集群 参考 # [1] 《04 | 架构风格：NewSQL和PGXC到底有啥不一样？》 王磊\n[2] opengauss系统架构\n[3] PolarDB Serverless: A Cloud Native Database for Disaggregated Data Centers\n[4] GaussDB(for Redis)揭秘：Redis存算分离架构最全解析\n[5] 阿里推出PolarFS分布式文件系统，存储与计算分开！附论文\n[6] Go to Page self\n[7] \u0026laquo;云原生数据库 原理与实践\u0026raquo; 5.2 5.3\n[10] VERBITSKI A，GUPTA A，SAHA D，et al. Amazon aurora：design considerations for high throughput cloud-native relational databases ［C］. Proceedings of the 2017 ACM International Conference on Man⁃ agement of Data，2017 ：1041-1052.\n"},{"id":12,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/SRE/%E6%95%85%E9%9A%9C%E6%A8%A1%E5%9E%8B/faultModel2/","title":"故障模型-中间件层","section":"故障模型","content":"\n故障模型-中间件层 # 故障模型-Runtime\u0026amp;Middleware\u0026amp;OS 负载均衡失效 数据库 数据库热点 数据库连接满 数据库宕机 数据库同步延迟 数据库主备延迟【参考2】 缓存 缓存热点【参考1】 缓存限流 OS资源 CPU抢占 案例 : HashMap并发访问，CPU100%【参考1】 案例：正则表达式回溯，CPU100% 内存抢占 案例：OOM killer 上下文切换 参考 # 大纲 # 超全总结 | 阿里如何应对电商故障？神秘演练细节曝光 阿里巴巴 周洋 Runtime \u0026amp; Middleware \u0026amp; OS # Go to Page self\nUCloud高可用数据库UDB主从复制延时的解决\n"},{"id":13,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/cqrs/","title":"CQRS 简介和案例分析","section":"应用架构","content":"\nCQRS全称是指Command Query ResponsibilitySeparation.CQRS的核心是一个简单的概念, 使用一个模型来读信息, 使用另一个模型来更新信息. 它是CQS原理在各个软件领域中的应用而产生的一种模式. CQRS把整个系统分成两个部分: 命令部分和查询部分. Command部分关注更新, Query部分关注读取.\n其实你可能早就接触过CQRS相关的概念,熟悉数据库的读者不会对索引陌生. Query部分:如果数据表有索引, 读数据表更加的快速. Command部分:如果数据表有index,update表时需要更新index, 所以update更加的慢.\n本文主要从CQRS在高伸缩性系统和领域驱动设计(DDD)两方面的应用阐述其优势。\nCQRS的出现有以下两种驱动力\n多参与者协作的环境 多个参与者会使用和修改相同的数据集. 参与者可以是行为人用户, 或者是软件. 数据总是过时的 在多协作的环境中, 数据一旦显示给了一个用户, 相同的数据可能已经被其它的参与者修改了, 说明数据已经过时了. 在哲学领域有一个命题, 你是否能踏进同一条河两次? 在多协作的环境中也有类似的问题, 你看到的数据总是过时的. 案例: 在查询出还有电影场次后, 你开始填自己的记录信息, 这时可能别人已经订购了你已经选择的座位, 或者这个时候, 一个事件到达银行说你信用卡有拖欠, 但最后你提交了这次订购，结果订购失败。\nCQRS与模型 # 在与command模型的交互中产生了事件, 顺序事件的累积可以捕获状态的所有变化, 这种交互模式称为事件源(Event Sourcing) .\n事件源(Event Sourcing)使得系统有了审计的功能, 回放事件可以使系统恢复到某个时间点的状态. 事件源(Event Sourcing)使command部分引入了异步的机制, 队列中的消息不需要马上处理, event handler可以异步的消费事件.当commands部分产生错误后, 直接向客户端回个错误并不友好, 这时可以引入回滚和重试机制. 在系统恢复正常之后, 队列中的消息重新发送并且用户接受到确认.\nQuery与Command两种行为的分离使得两个服务公用模型的分离也成为自然(图2)。单一模型(图1)分离成了两个模型:查询模型和命令模型.接口相应也分离成查询接口和命令接口. 客户端通过命令接口路由变化信息到命令模型. 查询模型和命令模型之间往往通过异步方式同步数据. 客户端通过查询接口读取查询模型以得到更新后的数据.\n但是模型在上下文中孤立的存在并不多见，更多模型之间会有相互的渗透，融合(图3)。共享内核表示了命令模型和查询模型之间重合的部分. (DDD) 在DDD领域中, 通用子系统可以代表更通用的服务. 在存储系统中, 通用子系统代表了在存储介质上的数据结构的融合, 公用.\n结合Event Souring 和模型共享内核来了解一下通用存储引擎的设计思路\n案例: BigTable和Cassandra的通用存储引擎\n数据写入时需要先写操作日志, 操作日志可以看成是Event Souring的持久化保存.成功后应用到内存中的MemTable中. 当内存中的MemTable达到一定大小, 需要将MemTable dump到磁盘中生成SSTable.由于数据同时存在MemTable和可能多个SSTable中, 读取操作需要按老到新合并SSTable和内存中的MemTable数据. 可以看到写操作对应的命令模型是MemTable, 读操作对应的查询模型是MemTable和多个SSTable,MemTable在读写时成为了共享模型, 以达到’提高写性能, 亦不降低读性能’的目的.\nCQRS与RESTFUL # 在REST风格的系统中, 资源动词, 名词, 表现三个维度上的分离, 形成了资源行为(统一接口), 资源状态, 资源表现形式. REST的6个约束中包括统一接口, 能够使客户端和服务端独立的演化。统一接口包括PUT, GET, POST等Http方法. PUT, POST类的接口可以归为command部分, GET 类的接口可以归为query部分. CQRS使得资源行为维度能够再分, 形成对服务层, 模型层, 数据存取层(DAO), 数据源层的纵向切分, 形成command和query两个子系统.REST统一接口是系统的水平接口，CQRS可以看成是系统的垂直接口。 在系统中, C和Q的分离可以看成是对系统中最粗粒度层次的划分.\n案例:Facebook缓存架构\n• 整体REST架构分成PUT(Query部分), POST(Command部分)两个部分. • Cache分Page cache, fragment cache, row cache, vector Cache, cache命中率见图。 • Page Cache和Fragment cache存放了API各种请求格式的数据，包括4种资源表现形式 XML, JSON, RSS, ATOM。 • 发表Tweets是先放入Kestrel, 再异步处理，Kestrel用的也是memcached协议。Kestrel可以看成Event Souring, Vector Cache是Command部分和Query部分之间的共享模型.\nCQRS与一致性 # 根据弱CAP原理，在分布式系统中，往往需要达到(一致性, 可用性,分区容忍性)三者的平衡，增强其中的一方就会削弱另外两方。在分布式系统中, P总是需要保证的, 所以需要在C和A之间做取舍. CQRS中的S(分离)隐喻了P, 即分区容忍性.\n贯彻CQRS的系统通过多种方式来实现各种级别的一致性，其中包括MS, MM(MMS, MMM), 两阶段提交, Paxos\n强一致性：假如A 先写入了一个值到存储系统，存储系统保证后续A，B,C的读取操作都将返回最新值。 弱一致性：假如A先写入了一个值到存储系统，存储系统不能保证后续A，B，C的读取操作能读取到最新值。 最终一致性：最终一致性是弱一致性的一种特例。假如A首先write了一个值到存储系统，存储系统保证如果在A，B，C后续读取之前没有其它写操作更新同样的值的话，最终所有的读取操作都会读取到A写入的最新值。\nMS # 在分布式系统中，通过读写多个数据副本来做到读写分离。 MS方式中, Master会承担起写请求(Command部分)的负载, Slave会承担起读请求(Query部分)的负载. 多个slave副本通过同步, 异步, 半同步的方式达到与Master数据的一致性.异步同步对延时和吞吐量这两个性能指标有好处. 在读多写少的系统中, 增加读的副本可以相对廉价的提高Query部分(读请求端)的水平可伸缩性. 如果有大量突增请求, 可以相应调高读的副本数.\u0026ndash;query部分的可伸缩性\nMM # Multi-master指一个系统存在多个master, 每个master都具有read-write能力，可以根据时间戳或业务逻辑合并版本。具备最终一致性。\n案例 BigTable: 同一个时刻同一个tablet只能被一台Tablet Server服务. 强一致性的分布式索引. GFS: MS实现的弱一致性分布式存储系统. Dynamo和Cassandra: MM实现的具备最终一致性的存储系统. 可能出现同一个key被多台机器操作的情况.多台机器上执行的顺序是无法保证的. 需要依赖基于vector lock的冲突合并方法解决冲突. 默认的解决方案是”last write wins”, 即在读的时候合并多个写者产生的多个版本的数据.\n\u0026ndash;To do\nCQRS与数据存储 # Command部分:　相对关注事务处理,持久化为关系结构数据. 在数据库中, 使用第3范式. Query部分: 相对关注性能. 使用反范式的方式来最小化数据的级联. 在数据库中, 可以使用第一范式, 也可以结合使用nosql技术.\n案例:　Mysql + Redies混合存储(sql + nosql混合存储)\nMySQL把数据同步到NoSQL中,这种架构适用于需要把数据同步到多种类型的存储中。 Nosql通过装做是mysql的slave, 从mysql同步数据.MySQL到NoSQL同步的实现可以使用MySQL UDF函数，MySQL binlog的解析来实现。\nCQRS与分布式事务 # 两阶段提交是实现分布式事务的常用方式, 协议比较通用. 但两阶段提交中所有事务序列化的通过master coordinator, 是吞吐率和延迟的杀手.\nCQRS是完全建立在BASE(Basic Availability, Soft-state, Eventual consistency)事务基础上的. 在CQRS实现中, 通过降低对写端的压力, 减少锁的竞争和死锁的可能, 来增加写端的性能.各种实现方式会有自定义的协议, 相对于两阶段提交灵活但不够通用.\n在对性能要求不高的系统中, 应该采用两阶段提交加快开发. 在对性能要求不是很高系统中, 应该考虑采用消息队列.\n案例1:　ebay分布式事务 消息队列上的CQRS + 消息应用状态表\n更新业务表A\n更新业务表B的事件放入消息队列\n提交事务1(包括步骤1, 2)\n查询队列中的消息, 更新业务表B.\n插入消息应用状态表message_applied\n提交事务2(包括步骤5, 6)\n如果上述事务成功之后, dequeue message\n删除消息应用状态表中的事件\n在关注第2点和第4点之后, 可以看到队列的插入(command)和查询(query) 放在了两个事务中.\n案例2: 淘宝分布式事务 日志表上的CQRS + 去重表\n更新业务表A\n更新业务表B的事件放入日志表, 并自动生成一个唯一的transactionID。\n提交事务1(包括步骤1, 2)\n消息中间件保证从主机1上读取更新业务表B的事件和transactionID, 并且这个消息路由到主机2.\n更新业务表B\n将transactionID插入去重表\n提交事务2(包括步骤5, 6)\n这两个实现方式, 它们有之间的共性, 就是有一个元素被分步骤的使用了CQRS.在案例1中是消息队列，案例2中是日志表. 从CQRS的角度看, 这两种实现方式没有本质的区别, 方式2可以看成是方式1的变体.\n案例2中的去重表等价于案例1中的消息应用状态表. 案例2看似复杂, 多了步骤4, 实际是保持事务1,2同步临界区的最小化, 等于是把案例1中查询队列中的消息(步骤4)剥离出事务, 防止不必要的查询错误导致回滚整个事务. 案例1的事务1牵涉到了业务表A和队列的混合型业务事务,实现复杂。案例2的两个事务都是数据库的系统事务, 可以使用两阶段提交, 相对通用. 案例2的去重表没有删除过, 能保证最终的消息都是已经成功的事务.案例1 有dequeue message步骤, 并在事务外, 在出现故障后, message_applied会留下一些垃圾内容. \u0026ndash;\nCQRS与CDN # 在大规模web站点中, 动态数据和静态数据(图片)的分离是优化的通用策略. CQRS能够在Query端做到极致的优化, 例如缓存, 分区, 备份(replication), 分布式的CDN. CDN是一种离用户相对近的边缘缓存, 能提高用户体验.\n***案例: *** 淘宝CDN\nCQRS原理 # CQRS来源于Bertrand Meyer提出的CQS原理。CQS原理从OOP中推导出来，大致是说如果你返回一个值你就不能改变状态。如果你改变了状态，你的返回值必须是void类型的。\nCQS原理其实在很多领域都有应用, 包括OOP中字段的的setter/getter,Java String(copy-on-write), Java thread(ConcurrentHashMap 读写分离锁), snapshot(copy-on-write), 数据库索引, 数据库sql(DML, DDL)中都可以看到CQS原理的影子.\n小结: # CQS中的分离(S)粒度,小到对象状态的setter和getter方法, 大到子系统的形成. 就如Greg谈到的， CQRS本身是个简单的小模式，有趣的是在结合两个服务时所要考虑的架构属性。 CQRS在复杂性管理和提高系统伸缩性有着独特的优势。\n参考: # clarified CQRS CQRS CQRS Documents by Greg Young NoSQL架构实践（一）——以NoSQL为辅 DDD \u0026ndash; domain driven design (共享内存) \u0026ndash; Even Eric Rethinking architecture with CQRS Twitter架构图(cache篇) Event Sourcing \u0026ndash; Martin fowler "},{"id":14,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/available/","title":"高可用 Available","section":"系统架构","content":"\n目录 # 原理 # CAP # CP系统: hbase, zookeeper AP系统: cassandra, eureka\nnPRT公式 [1] # 可以推导出风险期望的公式 # 控制风险的4大因素（nPRT） # 减少风险数量，n 降低风险变故障的概率（即：增加风险变故障的难度），P 减小故障影响范围，R 缩短故障影响时长，T 高可用架构设计的7大核心原则 # 少依赖原则：能不依赖的，尽可能不依赖，越少越好（n） 弱依赖原则：一定要依赖的，尽可能弱依赖，越弱越好（P） 分散原则：鸡蛋不要放一个篮子，分散风险（R） 均衡原则：均匀分散风险，避免不均衡（R） 隔离原则：控制风险不扩散，不放大（R） 无单点原则：要有冗余或其他版本,做到有路可退（T） 自我保护原则：少流血，牺牲一部分，保护另外一部分（P\u0026amp;R\u0026amp;T） 可用性 7 级图表 [成熟度] [5] # 当一个服务挂了的时候\n第一级：Crash with data corruption, destruction. 第二级：Crash with new data loss. 第三级：Crash without data loss. 数据高可用-冗余, destruction 测试 第四级：No crash, but with no or very limited service, low service quality.\n流控系统， eg. 秒杀流量漏斗 第五级：Partial or limited service, with good to medium service quality. 第六级：Failover with significant user visible delay, near full quality of service\n容灾，恢复慢 第七级：Failover with minimal to none user visible delay, near full quality\n异地容灾 高可用-研发 # 容量规划和评估 [7] # [chat] 容量规划和评估的概念和流程。 容量评估是评估系统需要应对的业务体量，包括请求量、高峰峰值等，可以根据历史数据或产品预估来进行。容量规划则是在系统设计时就要考虑容量问题，规划好系统能够抗多少的量级，涉及到系统架构设计和资源分配等问题。而性能压测则是为了确保容量规划的准确性，通过压测来测试系统的性能指标，如QPS和响应耗时，以确定系统是否能够承受实际业务流量。\n性能压测要关注的指标很多，但是重点要关注是两个指标，**一个是 QPS、一个是响应耗时，**要确保压测的结果符合预期。\nQPS 预估（漏斗型） [7] # [chat] QPS预估中的漏斗型预估方法。 漏斗型预估是根据请求的层面和模块来构建漏斗模型，预估每个层级的QPS量级，随着请求链路的下行，QPS量级会逐步减少。预估的层级包括服务、接口、分布式缓存等各个层面，最终构成完整的QPS漏斗模型。漏斗型预估方法可以帮助我们更准确地预估系统承载的QPS量级，从而做出更合理的容量规划和评估。\nQPS 预估（漏斗型）就是需要我们按照请求的层面和模块来构建我们的预估漏斗模型，然后预估好每一个层级的量级，包括但不限于从服务、接口、分布式缓存等各个层面来预估，最后构成我们完整的 QPS 漏斗模型。\n高可用-服务分层 # 分层解析 [6] # 接入层 [2] [R] # 地域\u0026amp;错误感知自动 failover 视 endpoint 健康度自动 failover 一定比例流量至其他可用区/地域，直至 endpoint 全部不健康时 100% 流量自动 failover 至其他可用区/地域。 地域感知流量分发 distribute eg. 上海一区和上海二区按照 80% 和 20% 的比例分发 服务层 应用层 [6] # 关注点 [7]\n无状态和负载均衡设计 弹性扩缩容设计 异步解耦和削峰设计（消息队列） 故障和容错设计 过载保护设计（限流、熔断、降级） 传统应用高可用\nCLB+CVM+AS 架构图 [pic] 应用实践 云原生应用部署\n涉及的产品 微服务平台 TSF API网关 TKE容器服务 云原生应用部署方案[pic] 应用的容灾设计 [pic 要重新看]\n单区域容灾 跨地域容灾 跨地域多活 业务拆分, 单元化部署 混合云部署 云上和IDC各部署一套完整的业务系统 异地多活set化部署 Unit由多个set组成 建议单写多读的架构 set不一定限制在一个机房，可跨机房、跨地域部署 系统中的高可用\nKubernetes 之 master高可用集群搭建 Redlock - redis分布式锁的高可用 百亿规模API网关服务Shepherd的设计与实现 服务隔离\n集群隔离 请求隔离 稳定性\n流量管控, 请求缓存, 超时管理, 熔断降级 中间件层 # kafka 高可用 zk高可用 系统中的高可用\n面试|图解 kafka 的高可用机制 isr 数据层 [3] [P] # 数据复制\n主从复制 同步复制，异步复制 复制日志的实现： 基于语句到复制， 基于wal的传输， 基于行的逻辑日志复制 eg. mysql， redis， hbase 复制滞后问题 多主复制 无主复制 一致性和共识 raft - etcd zab - zookeeper\n系统中的高可用\nMySQL 同步复制及高可用方案总结 MHA, MMM\n这可能是目前最全的Redis高可用技术解决方案总结 Master-slave, Cluster\n干货 | 阿里巴巴HBase高可用8年抗战回忆录\netcd - raft\n高可用-运营[7] # 灰度发布 # 监控+告警 # 安全性、防攻击设计 # 故障演练（混沌实验） # 接口拨测+巡检 # 参考 # 高可用的本质 云原生应用负载均衡系列 (2): 入口流量分发、容错与高可用调度 istio \u0026laquo;数据密集型应用系统设计\u0026raquo; 5章, 9章 \u0026laquo;亿级流量 网站架构核心技术\u0026raquo; 1.4 来自 Google 的高可用架构理念与实践 {% post_link \u0026rsquo;tencentTCP3\u0026rsquo; %} self 高可用架构和系统设计经验 腾讯 *** "},{"id":15,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/ddd/","title":"DDD  领域驱动设计","section":"应用架构","content":"\n目录 # 目标 # DDD的精髓是降低系统复杂度 规则 三个核心 # 统一语言 # 核心领域词汇表 统一语言重构迭代： 模型-》实现-》 重构-》 隐喻 -》 再到模型 命名规范 DSL - Domain Specific Language 领域划分 # 领域 子域 边界上下文（Bounded Context） 上下文映射（Context Mapping) 共享内核（Shared Kernel） 防腐层（Anti-Corruption）： 类似adaptor、facade， 对内部领域模型的隔离和屏蔽。 领域模型 # 抽象： 是从具体事物抽取、概括出它们共同的方面、本质属性与关系等。\n领域建模方法论： UML用例分析、 UML用例分析法 四色建模法 事件风暴\n模式 [3] # {% asset_img \u0026lsquo;ddd.png\u0026rsquo; %}\n实体 [4]\n可变性是实体的特点\n值对象 [4] 不变性是值对象的本质\nservice 领域服务是多个实体组合出来的一段业务逻辑\n聚合[5] 真实世界中整体与部分的关系 正是因为有这样的关系，在操作整体的时候，整体就封装了对部分的操作。 所谓的整体与部分的关系，就是当整体不存在时，部分就变得没有了意义。\n每个聚合对应一个Repo interface [7] 对聚合内的数据一致性负责[7] 聚合根 外部访问的唯一入口\n架构 # 六边形架构 # 又被称之为Ports and Adapters（端口和适配器架构）\n参考 # 《DDD（Domain Driven Design)的精髓》 直播+ppt 阿里张建飞 钉钉2020.05.21视频 xxx 领域驱动设计之领域模型 *** 《04 领域模型是如何指导程序设计的？》 DDD 微服务落地实战-拉钩专栏 《05 聚合、仓库与工厂：傻傻分不清楚》 DDD 微服务落地实战-拉钩专栏 {% post_link \u0026lsquo;DomainLogicAndSQL\u0026rsquo; %} self 资源 # 《实现领域驱动设计》 B *** 《领域专用语言实战》 B 没纸质 《领域驱动设计精粹》B 没纸质 《中台架构与实现 : 基于DDD和微服务》 B 没纸质 事件风暴和领域建模在阿里巴巴的落地实践 未 《DDD实战课》 《mksz541-DDD（领域驱动设计）思想解读及优秀实践~4》 V 《lg2061-DDD 微服务落地实战-拉钩专栏》 V *** 有代码 "},{"id":16,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/secKillSummary/","title":"秒杀系统总结","section":"系统设计","content":"\n参考： # 阿里大秒系统 秒杀系统架构优化思路 58沈剑 如何设计一个高可用、高并发秒杀系统 未 "},{"id":17,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/SRE/%E6%95%85%E9%9A%9C%E6%A8%A1%E5%9E%8B/faultModel3/","title":"故障模型-基础设施层","section":"故障模型","content":"\n故障模型-基础设施层 # 故障模型-Virtualization\u0026amp;Storage\u0026amp;Networking 服务器宕机\u0026amp;假死 断电 解决：异地多活 超卖 混和部署【3】 存储【2】 磁盘满，坏 不可写，不可读 网络【1】 网络抖动、丢包、超时 网卡满 DNS故障 断网 参考 # Virtualization \u0026amp; Storage \u0026amp; Networking # Kubernetes 网络疑难杂症排查分享 腾讯云 *** kubernetes 最佳实践：处理容器数据磁盘被写满 腾讯云 百度大规模战略性混部系统演进 "},{"id":18,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/ddd_practice/","title":"DDD-落地实战 Practice","section":"应用架构","content":"\nDDD 落地 # 基于DDD应用架构的核心 # 分离业务复杂度和技术复杂度\n设计思路 [4] # 贫血模型\n实现 业务逻辑放到Service中 缺点 [7] 业务逻辑被埋没在存储业务中 贫血模型的缺陷 [21] 无法保护模型对象的完整性和一致性 对象操作的可发现性极差 代码逻辑重复 代码的健壮性差 强依赖底层实现 99%的代码都是基于贫血模型 [21] 数据库思维 贫血模型“简单” 脚本思维 充血模型\n实现 业务逻辑放到领域对象中(实体对象中有实现方法) 开闭原则 保持了对象的封装性，使得领域模型在面临多态、继承等复杂结构时，易于变更 适用场景 类似继承、多态的情况 在软件设计的过程中需要将一些类型或者编码进行转换 更好地表现领域对象之间的关系 “聚合”，也就是在真实世界中那些代表整体与部分的事 比较\n贫血模型比充血模型更加简单易行 贫血模型 不需要 仓库、工厂、缓存，简单粗暴 充血模型需要更强的设计与协作能力 充血模型 需要开发人员有更强的OOA/D能力、分析业务、业务建模与设计能力 要有较强的团队协作能力 贫血模型 所有业务处理过程都交给Service完成 贫血模型更容易应对复杂的业务处理场景 分层 [2] # 用户接口层(Controller层) Application层 Domain层 Infrastructure层 代码分层 [2] # Interface\nassembler(DTO和领域对象的互转) dto facade（粗粒度的调用接口，将用户请求委派给一个或多个应用服务进行处理） Application\nevent（pub， sub）（事件处理相关的核心业务逻辑在领域层实现） service（应用服务） Domain\naggregate entity 聚合根 实体 值对象 工厂模式（Factory） event 事件实体以及与事件活动相关的业务逻辑代码 repository 所在聚合的查询或持久化领域对象的代码，通常包括仓储接口和仓储实现方法 Data Model只存在于数据层，而Domain Model在领域层，而链接了这两层的关键对象，就是Repository [7] service 领域服务是多个实体组合出来的一段业务逻辑 Infrastructure\nconfig Util（开发框架、消息、数据库、缓存、文件、总线、网关、第三方类库、通用算法等基础代码，） 项目代码[20] # 框架 # Axon Framework COLA [22] 参考 # 《13丨代码模型（上）：如何使用DDD设计微服务代码模型？》 欧创新 《04 领域模型是如何指导程序设计的？》 DDD 微服务落地实战-拉钩专栏 《24 直播：框架之上的业务分层》 体系课_Go高级工程师实战营(完结) 阿里技术专家详解DDD系列 第二讲 - 应用架构 refactor 之前的Transaction Script git refactor 之后的DDD 阿里技术专家详解DDD系列 第三讲 - Repository模式 COLA 4.0：应用架构的最佳实践 未 "},{"id":19,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/unifyModel/","title":"统一模型","section":"系统架构","content":"\n计算密集 # 计算密集 技术 产品 微服务 RPC(2th)Service Mesh(3th)\n多运行时(4th ) Dubboistio proxylessdaper 容器 编排 K8s Service Mesh Sidecar 控制面， 数据面 Envoy xDS 微软SMI 可观测 Tracing+Metric+Logs OpenTelemetry=\nOpenCensus+OpenTracing Sererless Sererless+ VMSererless+容器Sererless+服务Sererless+数据库 Ali ECSAli ECIFasSAurora，TiDB Cloud 数据密集 # 数据密集 技术 产品 消息队列 CloudEvent EventMesh 数据库 分离: 存算分离(资源伸缩)\n融合: HTAP(模型) TiDB(TiKV, TiFlash) ，PolarDB 大数据 流计算 Beam，Flink "},{"id":20,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/seckill/","title":"秒杀系统和商品详情页系统(培训讲义)","section":"系统设计","content":"\n秒杀系统和商品详情页系统(培训讲义)\n"},{"id":21,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/eai/","title":"应用集成方式","section":"应用架构","content":"\n企业应用之间主要有4种集成方式 # 文件传输 # 文件传输\n由各个应用产生文件, 其中包含提供其它应用使用的信息. 特征:　内部数据模式自由(schema free)\n优势: 1. 内部细节透明 2. 松耦合 3. 标准文件格式支持,如xml, json等.\n劣势: 1. 更新慢, 数据不同步, 数据过时 2. 数据不完整, 数据不完全正确时, 数据不一致问题解决困难.3. 产生大量小文件时, 昂贵且不可靠. 4. 语义不一致\n适用于批处理.\n共享数据库 # 共享数据库\n把应用的数据存储在一个共享数据库中来集成应用,　并定制数据库模式来处理不同应用的各种需求. 特征:　数据有模式(schema)\n优势: 1. 提供模型一致性 2. 通过事务管理数据一致性 3. 共享数据, 避免语义不一致问题.\n劣势: 1. 统一的数据库模式难设计. 模式改变,　应用也要改变. 2. 遗留系统很难提供一个可扩展的数据库模式供新的应用使用. 3.　性能瓶颈(单点访问) 4. 数据分布迁移困难. 5.应用和数据库紧耦合. 可作为新老系统的集成和改造的候选方案\n远程调用 # 远程调用 应用公开提供过程, 并能够被远程调用, 应用通过调用这些过程来执行操作并交换数据. 特征:　公开外部接口\n优势:　1. 提供语义一致性 2. 接口多样化, 有可兼容性并能扩展 3. 技术实现广泛, 如Java, .Net, CORBA, Web Services 4. 接口内部数据完整性和透明性.\n劣势:　１.　性能瓶颈,　不可靠(与本地访问相比)　２.　应用之间紧耦合, 可能会有时序上的耦合\n消息传递 # 消息传递 应用连接到一个公共的消息传递系统上,　并通过消息来交换数据和调用行为. 特征:　隐式调用, 完全隐藏接口\n优势:　1. 松耦合 2. 快速响应 3. 可靠 3. 通过消息转换解决语义不一致(实现方式: DDD 防腐层, ESB提供消息转换功能 )\n劣势: 1. 设计, 开发复杂(消息消费能力不够, 会引起消息的大量堆叠); 测试, 调试困难(可通过同步方式来测试) 2. 数据不完全同步 3. 有学习曲线\n权衡 # 耦合性和依赖性 # １.　消息传递和文件传输属于非直接耦合,　耦合性最低.\n２.　远程调用属于数据耦合,　耦合性次之.\n３.　共享数据库属于内容耦合,　耦合性最高.\n使用远程调用或共享数据库的应用之间是强依赖的关系\n使用消息传递或文件传输的应用之间是弱依赖的关系.\n修改性 # 共享数据库中的表结构一旦修改, 应用多少会做一些修改加以应对, 有的甚至是对整个应用的改造.\n数据格式 # 文件传输保持所产生的文件内容及格式不变就可以, 以文件作为公共接口, 内部格式可以不段变化. 远程调用, 共享数据库使用接口参数定义数据格式, 内部数据格式对外不可见. 共享数据库中数据格式的演化和扩展性相对于其它方式最弱.\n数据新鲜度 # 文件传输使用低频率的大文件传输会造成过时的信息, 有时可以容忍数据不一致, 但也可能造成灾难. 消息传递通过频繁和立即的发送数据来提高数据的新鲜度.\n数据序列化 # 数据需要序列化,反序列化. 格式可以是文本或者是二进制的. 传输的格式可以是json, xml或者PB.\n问题的解决 # 共享数据库和远程调用解决了文件传输语义不一致的问题. 消息传递相对于远程调用,共享数据库提高了性能和可靠性.\n模式之间的组合 # 消息传递和消息传递的组合(全异步化 SEDA) 消息传递和远程调用的组合(Half-Sync/Half-Async 半同步/半异步 POSA4) 远程调用和共享数据库的组合 ![(2) 变体, 应用之间有同步数据的问题, 实时或非实时同步]( \u0026ldquo;(2) 变体, 应用之间有同步数据的问题, 实时或非实时同步\u0026rdquo;)\n小结 # 4种方式, 每种模式都建立在前一种模式的基础之上, 以解决以前的集成方法所存在的问题. 相对于前一种模式也更抽象, 成熟度更高, 复杂度也更高.\n"},{"id":22,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/multiLive/","title":"异地多活 总结","section":"系统架构","content":"\n同城双活 [3] # 【跨机房写，同机房读】\n异地多活 [3] # 一般来说，数据同步的方案有两种：\n一种基于存储系统的主从复制，比如 MySQL 和 Redis。也就是在一个机房部署主库， 在异地机房部署从库，两者同步主从复制, 实现数据的同步。 另一种是基于消息队列的方式。一个机房产生写入请求后，会写一条消息到消息队列， 另一个机房的应用消费这条消息后，再执行业务处理逻辑，写入到存储服务中。 【异步方式同步数据】\n无论是采取哪种方案，数据从一个机房，传输到另一个机房都会有延迟，所以，你需要尽量 保证用户在读取自己的数据时，读取数据主库所在的机房。为了达到这一点，你需要对用户 做分片，让一个用户每次的读写都尽量在同一个机房中。同时，在数据读取和服务调用时， 也要尽量调用本机房的服务。\n【单元化，流量调度】\n总结 [3] # 不同机房的数据传输延迟，是造成多机房部署困难的主要原因，你需要知道，同城多机 房的延迟一般在 1ms~3ms，异地机房的延迟在 50ms 以下，而跨国机房的延迟在200ms 以下。\n同城多机房方案可以允许有跨机房数据写入的发生，但是数据的读取，和服务的调用应该尽量保证在同一个机房中。\n异地多活方案则应该避免跨机房同步的数据写入和读取，而是采取异步的方式，将数据从一个机房同步到另一个机房。\n案例 # 异地多活 阿里 【1】 基于Userid的单元化异地多活 主要改造整个交易链路 交易链路（单元）和非交易链路（中心）之间通过DRC同步数据。单元里的数据是全量、只读的 饿了么 【2】 思路+原则 基于地理位置的异地多活。用户、商家、骑手都会在相同的机房 可用性优先，放宽数据一致性 主要组件 GZS（元数据）+APIRouter（流量路由） SOA Proxy：内部网关、IDC之间调用 Data Replication Center：数据库复制、数据库和cache之间的一致性 Data Access Layer zk,mq在IDC之间的同步 数据一致性 [1][2] # 数据一致性 强一致场景 都读主节点 最终一致性场景 DRC异步同步数据 业务层异步分发数据 数据丢失 通过算法在不同机房都能生成相同的 参考 # 参考 # 《尽在双11:阿里巴巴技术演进与超越》 1.2节\n饿了么异地多活技术实现（一）总体介绍 饿了么框架工具部 知乎专栏\n《28 | 多机房部署：跨地域的分布式系统如何做？》 唐扬\nSET化架构设计 lql_h 未\n看完这篇异地多活的改造，我决定和架构师battle一下｜得物技术 未\n"},{"id":23,"href":"/www6vDistributed/docs/%E6%9E%B6%E6%9E%84/%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/domain_logic_and_sql/","title":"领域逻辑和SQL","section":"应用架构","content":"\n在企业应用中， 业务逻辑是复杂和庞杂的。 这些业务逻辑应该是被显示， 还是被隐藏， 这是一种选择。 在工作流中， 业务逻辑被从模块中剥离出来， 形成上层的粗粒度的业务流程。 在模块内部， 业务逻辑应该放在内存中， 还是在SQL中， 这也是一种选择。\n拨开项目的DAO层， 你可能会看到大量的SQL字符串， 业务逻辑就隐藏在这里。在有的项目你会看到hibernate之类的ORM框架， PO对象作为一种承载业务逻辑的机制。\n大量复杂的SQL拼接， 在SQL中放入业务逻辑与企业应用架构的分层原则相违背。OO的本质是抽象和分离， 各司其责。 领域逻辑更符合OO的精神。\n领域逻辑中的ORM不仅是一种可重用的对象装载方式， 也是一种虚拟化技术。对象与数据库的映射机制由ORM管控，对象装载和业务逻辑的分离， 表对于对象来说是透明的。就像JVM， 硬件对于Java工程师来说是透明的一样。\n在Domain Logic和SQL之间， 中庸的是Trasaction Script（事务脚本），它根据过程组织业务逻辑，每个过程处理来自表现层的一个单一请求。事务脚本简单的可以看成Domain Logic和Native SQL的结合。\nNative SQL的拥护者会提到SQL的性能优势。在多表查询中，Domain Logic确实没有Native SQL快。 在一条SQL即一个事务情况下，Native SQL会快些。 但在Domain Logic中， framework会使用cache做局部性的优化， 并且cache的对象是能够跨多个事务复用的（hibernate二级缓存）， 缓存策略也是可配置的。Framework并且提供Lazy load机制，在使用时加载对象， 进一步提升性能。\nDomain Logic可以在可理解的代码上做性能的改进，找到那占用了80%时间的20%的代码。 Native SQL可以在高性能的代码上做理解性的改进， 但sql中的逻辑是隐式的， 笔者认为要做到后者不易。 维护占到了软件生命周期的很大一部分， 应该先关注可修改性， 再关注性能。性能的第一原则是“不要提早优化”。\n在长生命周期的企业软件中，需求的迭代和代码的迭代是常态。 改变可能是人们把业务逻辑放在内存中， 使用Domain Logic的主要原因。\n过度复杂的sql拼接让人很难理解， 面条代码， 逻辑不连贯现象容易形成。 在Domain Logic的基础上， DSL（Domain Specific Language）提供了连贯接口， 业务清晰一目了然。 当然在理解程度上， 有人更适应SQL， 有人更适应Domain Logic， 这也是仁者见仁了。\nNative SQL中的sql的重用比Domain Logic中对象的重用更困难。 如果想重用一段SQL，在SQL中嵌入了判断逻辑，SQL的复杂度又就增加了。数据库视图是表的接口，可以定义一个视图， query重用定义好的视图。但视图有局限性， 只有select操作， 没有update操作， 如果有DML要求， 还需要定义存储过程。\n使用视图和存储过程提供的封装是不完全的。 在企业应用中，数据会来源于多个数据源， 多个数据库， xml文件，nosql数据库， 遗留系统等。 在这个情况中，数据存取的完全封装确实只能在应用的分层中实现。\nDomain Logic提供了抽象层次和模块化的机制， 对象装载和实际业务的分离， 好的对象装载机制会零入侵业务逻辑，如果Annotation用的是JSR标准， 把hibernate替换成OpenJPA也比较容易。\nDomain Logic要求对framework有好的把握， 一定的驾驭能力， 问题的解决能力。 Domain Logic里的对象有更多的约束条件，更多的模式， 比如一对一，多对一，多对多。 SQL相对更容易掌握， 代码直接可控。\n如果想要有可移植性，请不要使用sql。 各个语言都有自己的方言， 语法有略微的不同。Id的增长方式不同，有sequence， 有自增的， 有全局的。Mysql有limit关键字，oracle有rowid和rownum, db2有ROW_NUMBER() over(). 如果有数据库移植的需求， 已经写的Native SQL就会有大的改动。\n参考： # Domain Logic and SQL\n"},{"id":24,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/SRE/SRE/sreWorkbook/","title":"《SRE 工作手册》","section":"SRE","content":"\n参考 # SRE 实践的知识体系梳理\n"},{"id":25,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/SRE/SRE/sreWorkbookBasic/","title":"SRE 五大根基","section":"SRE","content":"\n目录 # SRE五大根基 # 实践SLO # 监控 # 告警 # 减少琐事 # 简单化 # 参考 # 《Google SRE工作手册》第二期SRE五大根基之一：SLO V *** 《Google SRE工作手册》第二期SRE五大根基之二：监控 V *** {% post_link \u0026lsquo;sreWorkbook\u0026rsquo; %} self {% post_link \u0026lsquo;sreWorkbookBasicSLO\u0026rsquo; %} "},{"id":26,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/SRE/SRE/sreWorkbookBasicSLO/","title":"SRE 五大根基-SLO","section":"SRE","content":"\n目录 # SRE 五大根基 之 SLO[1] # 步骤1. 制定SLO # 服务的SLO # VALET[Home Depot] Volume Avail Latency Errors Ticket 数据服务的SLO # Go to Page {% post_link \u0026lsquo;kafkaSLO\u0026rsquo; %} SLI # 服务类型 SLI类型 请求驱动 可用性，延迟，质量 流水线 时效性，正确率，覆盖率 存储 持久性 步骤2. 获得干系人认同 # SLO 仪表板[趋势] # 步骤3. 持续监控 改进SLO # 变更SLO 变更SLI实现 着手于现实的SLO 迭代 步骤4. 错误预算 SLO决策 # 基于SLO和错误预算的决策 # 步骤5. 进阶 # 用户旅程建模 依赖关系建模 参考 # 《Google SRE工作手册》第二期SRE五大根基之一：SLO V *** 《Google SRE工作手册》 第二章 "},{"id":27,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/SRE/SRE/sreWorkbookBasicAlert/","title":"SRE 五大根基-报警","section":"SRE","content":"\n告警设定考量 # 精准率\n减少误告警\n查全率\n减少漏告警\n检测用时\n过长 影响错误预算 重置用时\n过长 增长内存和IO开销 告警设定方法 # 基础 方法1 目标错误率 \u0026gt;= SLO阈值 window 方法2 延长报警时间窗口 方法3 延长告警触发前的持续时间 燃烧率 方法4 根据燃烧率发出告警 方法5 基于多个燃烧率的告警 方法6 基于多个窗口 多个燃烧率的告警 参考 # 《Google SRE工作手册》第四期基于SLO的告警配置及实践分享 V 《Google SRE工作手册》 第5章 "},{"id":28,"href":"/www6vDistributed/docs/%E6%80%A7%E8%83%BD/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/performancePool/","title":"性能优化-池化Pool","section":"性能优化","content":"\n目录 # 连接池优化 # 分类 # 数据库连接池 redis连接池 http连接池 数据库连接池 # 最小连接数， 最大连接数 建议最小连接数控制在 10 左右，最大连接数控制在 20～30 左右 连接的可用性\n使用连接发送**“select 1”**的命令给数据库看是否会抛出异常，如果抛出异常则将这个连接从连接池中移除，并且尝试关闭. 线程池 # JDK线程池 # 重要参数： coreThreadCount 和 maxThreadCount，\n[chat] 当大量请求进入线程池时，ThreadPoolExecutor 会根据其内部的线程池参数来处理请求。具体来说，当请求进入线程池时，ThreadPoolExecutor 会首先尝试将请求放入工作队列中，如果工作队列已满，则会根据线程池的配置来执行饱和策略，例如抛出异常、直接拒绝请求、阻塞等待或调用调用者运行等待任务。\n线程池中的线程会根据核心线程数和最大线程数的设置进行扩缩容。 当有新的任务提交到线程池时，如果当前线程数小于核心线程数，线程池会创建新的线程来处理该任务。 如果当前线程数已经达到核心线程数，线程池会将任务放入工作队列中，如果工作队列已满，则线程池会创建新的线程来处理该任务，直到达到最大线程数为止。 当线程池中的线程数量超过核心线程数时，空闲的线程会根据线程空闲时间的超时时间来自动销毁，以保证线程池中的线程数量不会无限制增长。 需要注意的是，在ThreadPoolExecutor中，线程池的线程数应该设置为适当的值，如果设置过小，则可能会导致线程饱和，影响任务的处理效率；如果设置过大，则会浪费系统资源，同时可能会导致线程竞争和锁的争用，影响系统的性能。因此，在使用ThreadPoolExecutor时，需要根据系统的负载和任务特性来合理地设置线程池的参数，以达到最佳的性能和效率。 [/chat]\nTomcat线程池 # Tomcat 使用的线程池就不是 JDK 原生的线程池，而是做了一些改造，当线程数超过 coreThreadCount 之后会优先创建线程，直到线程数到达maxThreadCount，这样就比较适合于 Web 系统大量 IO 操作的场景了。\n最佳实践 # 大量的任务堆积会占用大量的内存空间，一旦内存空间被占满就会频繁地触发 Full GC，造成服务不可用，我之前排查过的一次 GC 引起的宕机，起因就是系统中的一个线程池使用了无界队列.\n对象池 # Netty 对象池 [1][2] # netty为了避免过多的创建对象和频繁的gc使用了对象池，在需要创建ByteBuf的时候，从对象池中找，如果没有才会去创建一个新的ByteBuf。\nTomcat中的对象池 [3] # Tomcat 和 Jetty 都用到了对象池技术，这是因为处理一次 HTTP 请求的时间比较短，但是 这个过程中又需要创建大量复杂对象。\n参考 # Netty的对象池 Netty对象池 《20 | 总结：Tomcat和Jetty中的对象池技术》 《 07 | 池化技术：如何减少频繁创建数据库连接的性能损耗？》 唐扬 "},{"id":29,"href":"/www6vDistributed/docs/%E6%80%A7%E8%83%BD/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/performanceAnalysis/","title":"性能分析","section":"性能测试","content":"\n目录 # 方法 # 性能分析 7步法 [1] # 第一步：压力场景数据。\n第二步：分析架构图。\n第三步：拆分响应时间。\n第四步：全局监控分析。 [3]\n第五步：定向监控分析。 [3]\n第六步：判断性能瓶颈点。\n第七步：确定解决方案。\n性能测试场景 [2] # 基准场景\n容量场景\n稳定性场景\n异常场景\n工具 # Java性能分析 工具 btrace， 慢响应，生产用 排查占cpu最多的线程 JVM 工具 参考 # 《03 | 核心分析逻辑：所有的性能分析，靠这七步都能搞定》 高楼 《10 | 设计基准场景需要注意哪些关键点？ 》 高楼 《15丨性能测试场景：如何进行监控设计？》 *** 高楼 06丨倾囊相授：我毕生所学的性能分析思路都在这里了 高楼 未 "},{"id":30,"href":"/www6vDistributed/docs/%E6%80%A7%E8%83%BD/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/performanceTest/","title":"性能测试","section":"性能测试","content":"\nTPS和响应时间的关系[1] # 在这个图中，定义了三条曲线、三个区域、两个点以及三个状态描述。\n三条曲线：吞吐量的曲线（紫色）、使用率 / 用户数曲线（绿色）、响应时间曲线（深 蓝色）。 三个区域：轻负载区（Light Load）、重负载区（Heavy Load）、塌陷区（Buckle Zone）。 两个点：最优并发用户数（The Optimum Number of Concurrent Users）、最大并发 用户数（The Maximum Number of Concurrent Users）。 三个状态描述：资源饱和（Resource Saturated）、吞吐下降（Throughput Falling）、用户受影响（End Users Effected）。 性能指标的计算方式 [2] # 公式（1）：\n并发用户数计算的通用公式：C = nL/T\n其中 C 是平均的并发用户数；n 是 login session 的数量；L 是 login session 的平均长 度；T 指考察的时间段长度。\n公式（2）：\n并发用户数峰值：\nC’指并发用户数的峰值，C 就是公式（1）中得到的平均的并发用户数。该公式是假设用 户的 login session 产生符合泊松分布而估算得到的。\n仔细搜索之后发现会发现这两个公式的出处是 2004 年一个叫 Eric Man Wong 的人写的一篇名叫《Method for Estimating the Number of Concurrent Users》的文章。中英文我都反复看到很多篇。同时也会网上看到有些文章中把这个文章描述成“业界公认”的计算方法。\n性能测试 # 性能测试 环境假设 负载模型【4，5】 人为模拟请求 复制线上流量 引流 故障模拟【6】 Eg. tcpkill 结果分析 瓶颈分析 热点 【5】 20%代码影响了80% 类型【5】 benchmark 性能测试，负载测试，压力测试 稳定性测试 流程 [参考3] 定义响应时间，TP99 这个响应时间的限制下，找到最高的吞吐量（负载测试） 二步测试得到的吞吐量连续7天的不间断的压测系统（稳定性测试） 找到系统的极限值（压力测试，抗峰值 peek） Burst Test 注意点 平均值不靠谱，TP才靠谱(百分比分布统计) 平均值 标准方差 百分位数 中位数 关系 响应时间要和吞吐量挂钩 不同的吞吐量会有不同的响应时间 响应时间/吞吐量要和成功率挂钩 应用程序Profile 问题：让程序运行的性能变低 性能监控 # 性能优化 监控 分层 链路跟踪，APM 中间件监控 Eg.Tomcat 线程池 基础设施监控 Eg. cpu使用率，负载 数据可视化，可观察性 折线图 散点图 热图 告警通知 阈值 Eg. 比如大于TP99% 性能指标 响应时间，吞吐量，成功率【5】 低延迟，会有高吞吐 方法论【7-\u0026gt;2.5节】 工具法 USE，识别系统瓶颈 Utilization(使用率) Saturation(饱和度, 负载) Errors(错误) 工作负载特征归纳 延迟分析 静态性能调整 缓存调优 压测工具 # ab Jmeter wrk\n网络 # httpstat [20]\n参考 # 02丨性能综述：TPS和响应时间之间是什么关系？ 高楼 03丨性能综述：怎么理解TPS、QPS、RT、吞吐量这些性能指标？ 高楼 性能测试应该怎么做？ Go to Page self 关于容量预估/性能压测的思考 失效 Go to Page self 重复的 《性能之巅：洞悉系统、企业与云计算》 httpstat go httpstat python "},{"id":31,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/%E5%AE%B9%E9%87%8F%E4%BF%9D%E9%9A%9C/capacityGuarantee/","title":"容量保障与全链路压测","section":"容量保障","content":"\n容量保障Overview[2] # 大促容量保障的三项重点工作： 大促流量预估 大促容量测试 大促容量保障预案 容量预测 # 容量预测[1] # 我首先给出了**“皮尔逊相关系数”**这个工具，对服务 TPS 和 CPU 利用率之间的相关度进 行了定量分析，根据相关度的强弱，分别采取不同策略。其中，重点讲到了在两者弱相关 时的应对策略，如果能够穷举出尽可能多的相关特征，可以通过特征选取的方式对服务进 行画像，提升预测准确率；如果特征非常难找，那么可以依靠概率表的方式曲线救国。\n随着服务不断迭代，容量也在不断变化，我与你分析的第二个问题，就是如何平衡好服务 迭代和容量预测频率的关系。根据服务发布窗口（或其他变更时间点）建立滑动窗口机 制，既保证了在服务变更后能够尽快地更新模型，又不至于带来大量的计算量，是一个不 错的实践方式。\n业务场景变化也会导致容量变化，针对这个问题，我结合之前提到的全链路压测工作，通 过建立全链路压测和容量预测双向校准的机制，提前对变化的业务场景进行预测，识别容 量风险。\n全链路压测 # 核心功能[4] # 压测-部署架构 # 施压机的分布[3]\n大部分仍然是跟线上系统在同机房内，少量会在公有云节点上 以将全球（主要是国内）的 CDN 节点作为施压机 更加真实地模拟真实用户从全球节点进入的真实访问流量 成本过高，技术条件和细节难 压测的读写流量[3]\n读流量 写流量 对压测的写请求做专门的标记。 当请求要写数据库时，由分布式数据库的中间件框架中的逻辑来判断这个请求是否是压测请求，如果是压测写请求则路由到对应的影子库中，而不是直接写到线上正式的库中。 改造 全链路压测[5] # 数据隔离 物理隔离 vs. 逻辑隔离 [见表1] 中间件改造 eg. MQ改造 ​ Producer: 判断请求带压测标识，转换到数据体（msg）中 ​ Consumer: 判断数据（msg）中有压测标识， 恢复压测标识至请求中 应用服务改造 绕开限制逻辑 数据隔离前置 Mock 逻辑 ​ 表1 逻辑隔离 vs. 物理隔离\n隔离类型 逻辑隔离 物理隔离 中间件改造 小，几乎不需要改造 大，需要保证压测流量标识能一路透传不丢失 业务侵入性 大，会影响表结构设计 小， 对数据实体没有侵入 数据清洗难度 大， 需根据每个数据实体的标识单独定制清洗规则 小，压测数据都在影子表 可扩展性 弱， 新数据实体均需要设计新的压测标识 强， 流量标识为统一形式， 且与数据无关 安全性 弱， 与真实数据写入同一张表， 一旦隔离逻辑有疏漏， 会影响真实用户 强， 与真实数据分开存储， 即便考虑不周 ，也不会影响真实数据 参考 # 《09 | 容量预测（下）：为不同服务“画像”，提升容量预测准确性》 吴骏龙 《 13 | 大促容量保障体系建设：怎样做好大促活动的容量保障工作（下）》 吴骏龙 稳定性实践：容量规划之压测系统建设 极客时间？ 全链路压测体系建设方案的思考与实践 阿里 *** 《05 | 全链路压测：系统整体容量保障的“核武器”（上）》 吴骏龙 "},{"id":32,"href":"/www6vDistributed/docs/%E6%80%A7%E8%83%BD/Linux%E6%80%A7%E8%83%BD/linuxPerformance-cpu/","title":"Linux性能优化 之 cpu优化","section":"Linux性能","content":"\n一. 工具与指标 # 二. 最常用的cpu工具 # 最常用的cpu工具 工具中指标之间的关系\n三. cpu优化案例 # 1. 上下文切换的案例。 # 自愿上下文切换，是指进程无法获取所需资源，导致的上下文切换。比如说， I/O、内存等系统资源不足时，就会发生自愿上下文切换。 非自愿上下文切换，则是指进程由于时间片已到等原因，被系统强制调度，进而发生的上下文切换。比如说，大量进程都在争抢 CPU 时，就容易发生非自愿上下文切换。\n先用 vmstat ，查看了系统的上下文切换次数和中断次数；然后通过pidstat ，观察了进程的自愿上下文切换和非自愿上下文切换情况；最后通过 pidstat ，观察了线程的上下文切换情况，找出了上下文切换次数增多的根源，也就是我们的基准测试工具 sysbench。\n2. 进程 CPU 使用率升高的案例 # 可能代码里有死循环\n3. 不可中断进程和僵尸进程的案例 # 先用 top 观察到了 iowait 升高的问题，并发现了大量的不可中断进程和僵尸进程；接着用 dstat 发现是这是由磁盘读导致的，于是又通过 pidstat 找出了相关的进程。但我们用 strace 查看进程系统调用却失败了，最终还是用 perf 分析进程调用链，才发现根源在于磁盘直接 I/O 。 僵尸进程是因为父进程没有回收子进程的资源而出现的，那么，要解决掉它们，就要找到它们的根儿，也就是找出父进程，然后在父进程里解决。\n4.软中断的案例。 # 通过 top 观察到，系统的软中断 CPU 使用率升高；接着查看/proc/softirqs， 找到了几种变化速率较快的软中断；然后通过 sar 命令，发现是网络小包的问题，最后再用 tcpdump ，找出网络帧的类型和来源，确定是一个SYN FLOOD 攻击导致的。\n参考： # \u0026laquo;Linux性能优化实战 - 11 - 套路篇：如何迅速分析出系统CPU的瓶颈在哪里？\u0026raquo; 倪朋飞 \u0026laquo;Linux性能优化实战 - 04讲基础篇：经常说的CPU上下文切换是什么意思（下）\u0026raquo; 倪朋飞 "},{"id":33,"href":"/www6vDistributed/docs/%E6%80%A7%E8%83%BD/%E7%BD%91%E7%BB%9C/tcpTimewait/","title":"TIME_WAIT和优化","section":"网络","content":"\n故障现象 # 让我们先从一例线上故障说起。在一次升级线上应用服务之后，我们发现该服务的可用性变 得时好时坏，一段时间可以对外提供服务，一段时间突然又不可以， 大家都百思不得其解。 运维同学登录到服务所在的主机上，使用 netstat 命令查看后才发现，主机上有成千上万处 于 TIME_WAIT 状态的连接。\n经过层层剖析后，我们发现罪魁祸首就是 TIME_WAIT。为什么呢？我们这个应用服务需要 通过发起 TCP 连接对外提供服务。每个连接会占用一个本地端口，当在高并发的情况下， TIME_WAIT 状态的连接过多，多到把本机可用的端口耗尽，应用服务对外表现的症状，就 是不能正常工作了。当过了一段时间之后，处于 TIME_WAIT 的连接被系统回收并关闭 后，释放出本地端口可供使用，应用服务对外表现为，可以正常工作。这样周而复始，便会 出现了一会儿不可以，过一两分钟又可以正常工作的现象。\nTIME_WAIT概念和作用 # 概念 # 只有发起连接终止的一方会进入 TIME_WAIT 状态。 TIME_WAIT停留持续时间是固定的，是最长分节生命期 MSL（maximumsegment lifetime）的两倍（2MSL）。Linux系统里有一个硬编码的字段，名称为TCP_IMEWAIT_LEN，其值为 60 秒。也就是说，Linux 系统停留在 TIME_WAIT 的时间为固定的 60 秒。 作用 # 这样做是为了确保最后的 ACK 能让被动关闭方接收（可能丢失ACK后重传ACK），从而帮助其正常关闭。 第二个理由和连接“化身”和报文迷走有关系，为了让旧连接的重复分节在网络中自然消失。 TIME_WAIT的副作用和优化 # TCP和TIME_WAIT # 当连接的一方主动关闭连接，在接收到对端的 FIN 报文之后，主动关闭连接的一方会在 TIME_WAIT 这个状态里停留一段时间，这个时间大约为 2MSL。\n主动关闭连接的一方可以是客户端，也可以是服务端； 副作用1 主动关闭连接的一方 是客户端； 副作用2 主动关闭连接的一方 是服务端；\n副作用和优化 # 副作用1 对端口资源的占用，一个 TCP 连接至少消耗一个本地端口\n副作用1 优化 net.ipv4.tcp_tw_reuse（推荐） 从协议角度理解如果是安全可控的，可以复用处于 TIME_WAIT 的套接字为新的连接所用.\n只适用于连接发起方（C/S 模型中的客户端）； 对应的 TIME_WAIT 状态的连接创建时间超过 1 秒才可以被复用。 net.ipv4.tcp_max_tw_buckets（不推荐） 这个值默认为 18000，当系统中处于TIME_WAIT 的连接一旦超过这个值时，系统就会将所有的 TIME_WAIT 连接状态重置. 这个方法过于暴力，而且治标不治本，带来的问题远比解决的问题多，不推荐使用。\nSO_LINGER（不推荐）\n不要试图使用SO_LINGER设置套接字选项，跳过 TIME_WAIT；\n副作用2 服务器端程序重启时，地址被占用 -\u0026gt; Address already in use 异常 通过服务器端发起的关闭连接操作，引起了一个已有的 TCP 连接处于 TME_WAIT 状态，正是这个 TIME_WAIT 的连接，使得服务器重启时，继续绑定在127.0.0.1地址和xxxx端口上的操作，返回了Address already in use的错误。\n副作用2 优化 服务器端程序，都应该设置 SO_REUSEADDR 套接字选项，以便服务端程序可以在极短时间内复用同一个端口启动。\n大量TIME_WAIT的问题[gpt] # 问题 # RPC客户端A调用服务端C有很多time-wait，客户端B调用服务端C会有什么问题\n解决方案 # 在RPC客户端A调用服务端C时出现多个TIME_WAIT状态的情况下，如果RPC客户端B也调用相同的服务端C，可能会面临以下问题：\n1. 端口耗尽：每个TCP连接会占用一个本地端口号，如果RPC客户端A和B同时发起大量的请求，由于TIME_WAIT状态的存在，可能会导致本地端口耗尽，无法建立新的连接。\n2. 连接超时：由于服务端C处于TIME_WAIT状态，即使RPC客户端B发起请求，也需要等待TIME_WAIT状态消失后才能建立连接，这会导致连接的延迟增加，可能会触发连接超时，导致B无法正常与服务端C建立连接。\n3. 并发性能下降：当RPC客户端A和B同时发起大量请求时，如果每个请求都需要等待TIME_WAIT状态消失后才能建立连接，会导致连接的建立速度变慢，从而降低系统的并发性能。\n为了避免这些问题，可以采取以下措施： 1. 避免端口耗尽：可以通过增加可用端口数、增加端口重用的时间等方式来缓解端口耗尽的问题。在操作系统级别上，可以通过调整端口范围或增加可用端口的数量来解决。\n2. 优化连接复用：可以使用连接池来管理TCP连接，以便复用连接，避免频繁地创建和关闭连接。这样可以减少TIME_WAIT状态的产生，并提高连接的复用率和性能。\n3. 调整TIME_WAIT时间：可以在服务端C上调整TIME_WAIT状态的时间，在合理的范围内降低TIME_WAIT状态的持续时间，从而缩短等待时间，提高连接的可用性和性能。\n总结: 最佳的解决方案取决于具体的应用场景和系统配置，需要综合考虑性能、资源消耗和可维护性等因素来做出合理的决策。\n参考 # 网络编程实战 - 10 | TIME_WAIT：隐藏在细节下的魔鬼 盛延敏 网络编程实战 - 15 | 怎么老是出现“地址已经被使用”？ 盛延敏 "},{"id":34,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/%E7%A8%B3%E5%AE%9A%E6%80%A7/tcpFault/","title":"TCP故障模式","section":"稳定性","content":"\n一. 故障模式 # 对端无FIN包发送 # 网络中断 read TIMEOUT（setsockopt）\n先write再read， 持续重传直至TIMEOUT。 之后再write()， 返回SIGPIPE信号（Broken Pipe）。\nwrite Unreachable\n系统奔溃（如断电） read()或者write()持续重传直至TIMEOUT\n对端重启，read()或者write()返回RST。 read（）调用返回Connection Reset。write()返回SIGPIPE信号（Broken Pipe）。\n对端有FIN包发送（如程序奔溃） # read直接感知FIN\n没有read\n如果不read没办法得到TCP对端的响应.\n通过write()产生RST， read（）感知RST（Connection reset by peer）。\n向一个关闭连接连续写导致SIGPIPE信号（Broken Pipe）。\n二. Java对应的错误 # java.net.SocketTimeoutException # java.net.SocketException: Connection reset/Connect reset by peer: Socket write error # 指连接被重置。这里有两种情况，分别对应两种错误：第一种情况是通信的一方已经将Socket 关闭，可能是主动关闭或者是因为异常退出， 这时如果通信的另一方还在写数据，就会触发这个异常**（Connect reset by peer）； 如果对方还在尝试从 TCP 连接中读数据**，则会抛出 Connection reset 异常。\n为了避免这些异常发生，在编写网络通信程序时要确保： 程序退出前要主动关闭所有的网络连接。 检测通信的另一方的关闭连接操作，当发现另一方关闭连接后自己也要关闭该连接。\njava.net.SocketException: Broken pipe # 指通信管道已坏。发生这个异常的场景是，通信的一方在收到“Connect reset by peer:Socket write error”后，如果再继续写数据则会抛出 Broken pipe 异常，解决方法同上。\n参考: # 网络编程实战 - 17 | TCP并不总是“可靠”的？ 盛延敏 操作系统的视角 深入拆解Tomcat \u0026amp; Jetty - 38 | Tomcat拒绝连接原因分析及网络优化 李号双 Java的视角 "},{"id":35,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/%E7%A8%B3%E5%AE%9A%E6%80%A7/crashDetect/","title":"宕机检测-Lease、心跳","section":"稳定性","content":"\nLease # Kubernetes: 引入了一个新的 build-in Lease API[2]\n心跳 # 模式： ping-ping模式, ping-pong模式\nraft： 心跳选主\n参考 # 【分布式系统工程实现】如何检测一台机器是否宕机？ 阿里中间件 *** 当 K8s 集群达到万级规模，阿里巴巴如何解决系统各组件性能问题？ 曾凡松（逐灵） *** 《面向模式的软件架构-卷4》 20.15节 "},{"id":36,"href":"/www6vDistributed/docs/%E7%A8%B3%E5%AE%9A%E6%80%A7/%E6%B7%B7%E6%B2%8C%E5%B7%A5%E7%A8%8B/chaosEngineering/","title":"混沌工程","section":"混沌工程","content":"\n混沌工程实践[1,2] # 原则 # 成熟度 # 混沌工程实施步骤 # 制订混沌实验计划 定义系统稳态指标 做出系统容错行为假设 执行混沌实验 检查系统稳态指标 记录\u0026amp;恢复混沌实验 修复发现的问题 自动化持续进行验证 混沌工程产品 # 相关产品ChaosBlade 故障注入 参考 # 分布式服务架构下的混沌工程实践 阿里 穹谷 *** 混沌工程介绍与实践 阿里 穹谷 Netflix 混沌工程手册 Part 1：混沌工程简介 Netflix 混沌工程手册 Part 2：混沌工程原则 Netflix 混沌工程手册 Part 3：实践方法 "},{"id":37,"href":"/www6vDistributed/docs/%E6%80%A7%E8%83%BD/Linux%E6%80%A7%E8%83%BD/linuxPerformance/","title":"Linux性能优化","section":"Linux性能","content":"\n最常用的cpu工具 # memory tool # IO tool # Buffer是对磁盘数据的缓存，而Cache是文件数据的缓存，它们既会用在读请求中，也会用在写请求中。\n参考 # \u0026laquo;Linux性能优化实战 11 - 套路篇：如何迅速分析出系统CPU的瓶颈在哪里？\u0026raquo; 倪朋飞 \u0026laquo;Linux性能优化实战 21 - 套路篇：如何“快准狠”找到系统内存的问题？\u0026raquo; 倪朋飞 \u0026laquo;Linux性能优化实战 30 - 套路篇：如何迅速分析出系统IO的瓶颈在哪里？\u0026raquo; 倪朋飞 \u0026laquo;Linux性能优化实战 16 - 基础篇：怎么理解内存中的Buffer和Cache？\u0026raquo; 倪朋飞 "},{"id":38,"href":"/www6vDistributed/docs/%E6%80%A7%E8%83%BD/%E7%BD%91%E7%BB%9C/tcpUdpControlCongestion/","title":"TCP流控和拥塞控制","section":"网络","content":"\nTCP拥塞控制 # TCP拥塞控制-快速重传 # 快速重传 # 拥塞窗口是为了怕把网络塞满，在出现丢包的时候减少发送速度. 滑动窗口就是为了怕把接收方塞满，而控制发送速度.\n参考: # TCP 的那些事儿（下） （四）：网络性能排查之TCP重传与重复ACK 怎么让不可靠的UDP可靠？ 趣谈Linux操作系统 - 45-发送网络包（上）：如何表达我们想让合作伙伴做什么？ "},{"id":39,"href":"/www6vDistributed/docs/%E4%B8%80%E8%87%B4%E6%80%A7/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/raft/","title":"Raft协议","section":"分布式协议","content":"\n总结 # 从本质上说，Raft 算法是通过一切以领导者为准的方式，实现一系列值的共识和各节点日志的一致.\nRaft： leader + term + peers\nRaft-分区脑裂（成员变更的问题） # 分区脑裂[非majority有uncommited log、 term1]\n分区脑裂[majority可以同步log、 term2]\nRaft-领导者选举 # Raft-复制日志 # 副本数据是以日志的形式存在的，其中日志项中的指令表示用户指定的数据。 Raft 是通过以领导者的日志为准，来实现日志的一致的。 在 Raft 中日志必须是连续的 日志完整性最高的节点才能当选领导者 参考 # raft 动画 good The Raft Consensus Algorithm good 动画 各种系统实现 未 Raft Distributed Consensus Algorithm Visualization 动画 未 Raft对比ZAB协议 raft协议和zab协议有啥区别？ 一张图看懂Raft 未 Raft 为什么是更易理解的分布式一致性算法 未 分布式协议与算法实战 - 07 | Raft算法（一）：如何选举领导者？ 韩健 *** 分布式协议与算法实战 - 08丨Raft算法（二）：如何复制日志？.pdf 韩健 *** Raft 分布式系统一致性协议探讨 腾讯 未 论文 Raft一致性算法论文译文 In Search of an Understandable Consensus Algorithm(Extended Version) raft "},{"id":40,"href":"/www6vDistributed/docs/%E6%80%A7%E8%83%BD/Linux%E6%80%A7%E8%83%BD/linuxProfile/","title":"Linux性能分析","section":"Linux性能","content":"\nLinux observability tools | Linux 性能观测工具 # Linux性能观测工具\n快速性能诊断(快速体检) # 1. 系统平均负载 # # 0.25- 1分钟负载 ， 0.22-5分钟负载， 0.23-15分钟负载\r[root@10-25-3-55 /]# uptime\r23:02:19 up 285 days, 11:37, 1 user, load average: 0.25, 0.22, 0.23\r# 如果cpu个数是4， 则平均负载4是合理的。\r[root@10-23-25-248]$grep \u0026#39;model name\u0026#39; /proc/cpuinfo | wc -l\r4 平均负载: 单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是单位时间内的活跃进程数。\n场景 CPU 密集型进程，使用大量 CPU 会导致平均负载升高，此时这两者是一致的； I/O 密集型进程，等待 I/O 也会导致平均负载升高，但 CPU 使用率不一定很高； 大量等待 CPU 的进程调度也会导致平均负载升高，此时的CPU使用率也会比较高。\n2. dmesg | tail : 系统信息 导致性能问题的错误 # [9927609.690053] ffmpeg invoked oom-killer: gfp_mask=0x201da, order=0, oom_score_adj=0\r[9927609.690106] [\u0026lt;ffffffff81184c7e\u0026gt;] oom_kill_process+0x24e/0x3c0\r[9927609.690109] [\u0026lt;ffffffff8118471d\u0026gt;] ? oom_unkillable_task+0xcd/0x120 3. vmstat 1 : 虚拟内存 # 进程(饱和度) 内存 CPU\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\rr b swpd free buff cache si so bi bo in cs us sy id wa st\r8 0 0 402204 303208 8686376 0 0 2 10 0 0 5 6 89 0 0 4. mpstat -P ALL 1 : CPU分解时间 # 11:08:02 PM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle\r11:08:03 PM all 28.68 0.00 60.85 0.00 0.00 0.00 0.00 0.00 0.00 10.47\r11:08:03 PM 0 28.43 0.00 60.78 0.00 0.00 0.00 0.00 0.00 0.00 10.78\r11:08:03 PM 1 28.71 0.00 61.39 0.00 0.00 0.00 0.00 0.00 0.00 9.90\r11:08:03 PM 2 29.00 0.00 59.00 0.00 0.00 0.00 0.00 0.00 0.00 12.00\r11:08:03 PM 3 28.71 0.00 61.39 0.00 0.00 0.00 0.00 0.00 0.00 9.90 5. pidstat 1 ： 每个进程的统计摘要 # 11:11:14 PM UID PID %usr %system %guest %CPU CPU Command\r11:11:15 PM 0 8715 1.00 0.00 0.00 1.00 3 pidstat\r11:11:15 PM 0 27930 0.00 1.00 0.00 1.00 2 java\r11:11:15 PM 0 28042 1.00 0.00 0.00 1.00 1 java\r11:11:15 PM 0 28044 1.00 0.00 0.00 1.00 1 java\rAverage: UID PID %usr %system %guest %CPU CPU Command\rAverage: 0 23 0.00 0.07 0.00 0.07 - ksoftirqd/3\rAverage: 38 531 0.00 0.07 0.00 0.07 - ntpd\rAverage: 0 2642 0.07 0.00 0.00 0.07 - aliyun-service\rAverage: 27 2784 0.00 0.07 0.00 0.07 - mysqld\rAverage: 0 7462 0.73 0.20 0.00 0.93 - java 6. iostat -xz 1 ： 磁盘 # avg-cpu: %user %nice %system %iowait %steal %idle\r29.15 0.00 59.30 0.25 0.00 11.31\rDevice: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %util\rvda 0.00 0.00 1.00 1.00 8.00 4.00 12.00 0.05 26.50 53.00 0.00 26.50 5.30 7. free -m ： 内存 # buffer， cache， Swap\rtotal used free shared buff/cache available\rMem: 32013 22842 391 1 8779 8723\rSwap: 0 0 0 8. sar -n DEV 1 : 网路吞吐 # Average: IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s\rAverage: vethd4593b4 0.00 0.00 0.00 0.00 0.00 0.00 0.00\rAverage: eth0 1.67 1.52 0.17 1.66 0.00 0.00 0.00\rAverage: eth1 2.74 75.08 0.16 8.86 0.00 0.00 0.00\rAverage: lo 4.86 4.86 0.71 0.71 0.00 0.00 0.00\rAverage: docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 9. sar -n TCP,ETCP 1 : TCP指标 # 10. top ： 变化的负载的汇总 # 参考 # 性能 # 超全整理！Linux性能分析工具汇总合集 60,000毫秒内对Linux的性能诊断 28个UNIX/LINUX的命令行神器 "},{"id":41,"href":"/www6vDistributed/docs/%E6%80%A7%E8%83%BD/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/performance/","title":"性能优化总结 *","section":"性能优化","content":"\n关键词 # 锁优化，池化，数据库优化，架构优化， 系统优化，性能测试， 监控\n应用优化 # 应用\n锁 【4】 粒度 粗化 最小化锁范围 Eg， 单线程写文件 锁拆分，分散锁 减少竞争，race condition eg. ConcurrentHashMap，LongAdder 分离锁 读写锁 读多写少的场景 队头队尾， 两把锁 乐观锁 CAS 并行 【1】 多线程 fork-join模式【2】 本地化 Eg. ThreadLocal Actor Eg. Akka CSP Eg. Goroutie 函数式范性 不可变对象 单核单线程 Eg. Redis, Nginx 池化(重用) Eg. 线程池，数据库连接池 代码调优 字符串操作 多线程调优 锁 【4】 线程个数【9】 异步操作 【8】 简化代码 热点优化 【5】 数值精度 Eg. 双精度 单精度 算法优化 # 算法\n算法调优 分而治之 【6】 fork-join【2】 Map-Reduce 哈希算法 预处理 提前计算(预处理)，最后合并 算法和数据结构 算法复杂度 大O表示【10】 时间复杂度 O(1) 数组访问 栈、链表的插入/删除 Hash Table O(log(n)) 二叉搜索树 自平衡二叉搜索树 AVL树 红黑树 B树(多路树) O(n log(n)) 快排、归并、堆 快排(分治算法【6】) O(n) O(n^2) 选择、插入、冒泡 数据结构 树，链表，栈，队列 通用优化 # 通用方法\n异步化 【8】 消息 进程内 Eg. Disruptor, EventBus Broker 事件 Eg. EventSource 服务异步化 Eg. 异步网关 Batch Eg. redis pipeline buffer io Copy on Write Eg. mysql MVCC, CopyOnWriteArrayList 数据库优化 # 索引优化 大表优化 水平拆分 垂直拆分 慢SQL优化 乐观锁 架构/系统优化 # 架构/系统优化\n弹性/伸缩 资源调度 Eg. k8s 资源扩容 可扩展 垂直拆分 水平拆分 分布式 并行 【1】 延迟 Eg. 跨IDC网络延迟 上海\u0026lt;-\u0026gt;北京 50ms 节省空间/时间换空间【8、11】 不存储，重新计算 稀疏数据结构 Eg. 稀疏矩阵 数据压缩传输 RSYNC 的核心算法 Huffman 编码压缩算法 动态分配策略 Eg. ArrayList(10)，HashMap(16)的动态扩展 垃圾回收 空间换时间 缓存 数据冗余，replication 系统结构 大型系统分解成模块 “粗略估算”，性能分析 系统软件 替换更快的操作系统、中间件、数据库、编译器 Linux系统优化 # 系统\n文件系统 pageCache, 预读 Eg. Rocketmq 顺序写，随机读 大块读优于小块读 CPU CPU绑定 CPU上下文切换 Eg. Redis单线程 CPU不是Redis的瓶颈， 瓶颈可能是内存的大小或者网络带宽 CPU缓存 问题：伪共享 解决：cacheline padding 网络I/O NIO,非阻塞I/O，epoll 协议调优 TCP参数 内存 zeroCopy HugePage 参考 # 高性能高并发系统的稳定性保障 京东 Linux性能优化实战 极客时间 性能测试应该怎么做？ coolshell deleted Go to Page self deleted 关于容量预估/性能压测的思考 deleted 失效 Go to Page self 重复的 deleted xxx 性能调优攻略 coolshell *** Go to Page self 每个程序员都应该收藏的算法复杂度速查表 《编程珠玑 第2版》 wordcount设计与优化 竞赛题 其他 # latency 未 Go to Page self 未 Go to Page self 未 "},{"id":42,"href":"/www6vDistributed/docs/%E4%B8%80%E8%87%B4%E6%80%A7/idempotent/","title":"幂等","section":"一致性","content":"\n三种实现方式： # 1. 利用数据库的唯一约束实现幂等 # 在数据库中建一张转账流水表，这个表有三个字段：转账单 ID、账户 ID 和变更金额，然后给转账单 ID 和账户 ID 这两个字段联合起来创建一个唯一约束，这样对于相同的转账单 ID 和账户 ID，表里至多只能存在一条记录。\n基于这个思路，不光是可以使用关系型数据库，只要是支持类似**“INSERT IF NOT EXIST”语义**的存储类系统都可以用于实现幂等，比如，你可以用 Redis 的 SETNX 命令来替代数据库中的唯一约束，来实现幂等消费。\n2. 为更新的数据设置前置条件 # “将账户 X 的余额增加 100 元”这个操作并不满足幂等性，我们可以把这个操作加上一个前置条件，变为：“如果账户 X 当前的余额为 500 元，将余额加100 元”，这个操作就具备了幂等性。\n更加通用的方法， 数据增加一个版本号属性，每次更数据前，比较当前数据的版本号是否和消息中的版本号一致，如果不一致就拒绝更新数据，更新数据的同时将版本号 +1，一样可以实现幂等更新。\n3. 记录并检查操作 # 通用性最强，适用范围最广的实现幂等性方法：记录并检查操作，也称为“Token 机制或者 GUID（全局唯一 ID）机制”。\n具体的实现方法是，在发送消息时，给每条消息指定一个全局唯一的 ID，消费时，先根据这个 ID 检查这条消息是否有被消费过，如果没有消费过，才更新数据，然后将消费状态置为已消费。\n这种方法适用范围最广，但是实现难度和复杂度也比较高，一般不推荐使用。\n参考： # 分布式系统互斥性与幂等性问题的分析与解决 蒋谞 消息总线真的能保证幂等？ 58沈剑 《微服务设计》 11.6节 Sam Newman 《消息队列高手课 - 如何处理消费过程中的重复消息？》 李玥 "},{"id":43,"href":"/www6vDistributed/docs/%E4%B8%80%E8%87%B4%E6%80%A7/splitBrain/","title":"Split Brain","section":"一致性","content":"\n关键词: 脑裂, fence, Quorums , epoch\n脑裂： 类似 CAP中的P Partition tolerance(分区容错性): 网络分区发生时，一致性和可用性两难全\n一. 通用解决方案 # Quorums Redundant communications，冗余通信的方式 Fencing 二. 系统 # / 现象 解决方案 kafka kafka脑裂现象:1. 存在多个controller 2. consumer的splitBrain controller使用epoch来避免脑裂 elastic search 配置discovery.zen.minimum_master_nodes，类似Quorums zookeeper 两个leader[3] Quorums leader单调递增的epoch raft脑裂 两个majority [2] Quorums + term redis脑裂、mysql脑裂\n参考: # redis 脑裂等极端情况分析 Redis Cluster is not able to guarantee strong consistency. / In general Redis + Sentinel as a whole are a an eventually consistent system\nraft协议 self 脑裂是什么？Zookeeper是如何解决的？ ​\n​ ​\n"},{"id":44,"href":"/www6vDistributed/docs/%E4%B8%80%E8%87%B4%E6%80%A7/consistent/","title":"分布式一致性 总结","section":"一致性","content":"\n分布式一致性 # [粉色-Unavailable] 在某些网络故障情况下不可用。为了确保安全，一些或所有节点必须暂停操作。\n[黄色-Sticky Available] 只要客户端只与相同的服务器通信而不切换到新的服务器，就可在每个非故障节点上使用。\n[蓝色-Total Available] 即使网络完全瘫痪，也可在每个非故障节点上使用。\n一致性 # 强一致性模型 # 强一致性 协议 特性 工程 线性一致性[chat] 2PC\n3PC #1 延迟大，吞吐低。全局锁资源 JTA(XA)\n{% post_link \u0026rsquo;transactionSeata\u0026rsquo; Seata XA,AT 非入侵 %} self 顺序一致性[chat] Paxos #1 难理解，延迟大，吞吐中等，全局锁资源 Google Chubby 顺序一致性 {% post_link \u0026lsquo;zookeeperZab\u0026rsquo; Zab %} self\n逻辑时钟 类似多线程程序执行顺序的模型 Zookeeper的读 1.两个主流程，三个阶段 2. Quorum:2f+1个节点，允许f个节点失败 强一致性 {% post_link \u0026lsquo;raft\u0026rsquo; %} self 相对Paxos简单。主从，三个阶段 Go to Page self 逻辑时钟\nLamport提出逻辑时钟是为了解决分布式系统中的时序问题，即如何定义a在b之前发生. Java中有happen-before 图2. 逻辑时钟 logic-clock\n线性一致性 Linearizability\n线性一致性 #1： 严格一致性（Strict Consistency）或者原子一致性（Atomic Consistency） 一个操作对于系统的其他部分是不可中断的\n顺序一致性 Sequential\n任何一次读写操作都是按照某种特定的顺序。 所有进程看到的读写操作顺序都保持一致。\n顺序一致性虽然通过逻辑时钟保证所有进程保持一致的读写操作顺序，但这些读写操作的顺序跟实际上发生的顺序并不一定一致。而线性一致性是严格保证跟实际发生的顺序一致的。 Paxos、ZAB 和 RAFT 有以下几个主要的共同点[Claude]:\n都通过选举 Leader 来接受客户端请求, Leader 接收写操作,然后同步给 Follower 节点,保持集群数据的一致性。 都使用**日志(log)**来记录节点状态的变更,Follower 节点通过应用相同的日志来保持数据一致。 都通过多个阶段来实现一致性,例如Prepare 阶段 和 Commit 阶段。 都需要超过半数以上的节点达成一致(quorum),才能提交日志。 弱一致性模型 # 因果一致性 # 因果一致性 协议 特性 工程 因果一致性 向量时钟 Vector clock[向量时钟] 图1 微信朋友圈的评论, Dynamo 向量时钟 图1. 向量时钟 vector-clock\n客户端为中心的一致性（Client-centric Consistency） # 客户端为中心的一致性\n最终一致性 以客户端为中心的一致性为单一客户端提供一致性保证，保证该客户端对数据存储的访问的一致性，但是它不为不同客户端的并发访问提供任何一致性保证. 类型\n单调读一致性（Monotonic-read Consistency）\n**kafka的消费者的单调读 ** 单调写一致性（Monotonic-write Consistency） 读写一致性（Read-your-writes Consistency） 写读一致性（Writes-follow-reads Consistency） 最终一致性 # 最终一致性 协议 特性 工程 反熵Anti-Entropy\nGossip Cassandra， redis的集群状态的同步机制 Sloppy quorum # Sloppy quorum 特性 工程 R+W\u0026gt;N[ReadQurum-WriteQurum] 可定制 Dynamo, Cassandra 定制灵活 最终一致性-工程 # TCC # TCC 流程 1.主流程控制整个事务\n2.分流程提供Confirm和Cancel方法。 阶段 Try: 阶段1的业务执行\nConfirm: 阶段2的业务执行\nCancel: 回滚Try阶段执行的业务流程和数据 TCC FMT\n{% post_link \u0026rsquo;transactionSeata\u0026rsquo; Seata TCC %} self 基于事务消息的分布式事务 # EBay模式 [8]\n正向流程\n[本地事务+幂等业务接口+half消息] 消息状态\n初始化：消息为待处理状态\n业务成功：消息为待发送状态\n业务失败：消息删除 反向流程（异常流程，补偿流程）\n中间件询问业务执行结果，更新消息状态 工程 {% post_link \u0026lsquo;mqRocketmqTransaction\u0026rsquo; RocketMQ事务消息 %} self\n基于本地消息的分布式事务 # Saga流程 # Saga 1PC (一阶段) 基于补偿的消息驱动的用于解决long-running process业务。 工程\n{% post_link \u0026rsquo;transactionSeata\u0026rsquo; Seata Saga %} self 弱一致性-工程 # 补偿 # 流程\n状态查询（成功or失败）+补偿 流程细节\n定时校验异常 + 补偿 State Machine \u0026amp;\u0026amp; Primary-copy # state machine replication \u0026amp;\u0026amp; primary-copy\n复制状态机(state machine replication) 多个节点上，从相同的初始状态开始，执行相同的一串命令，产生相同的最终状态\n状态机 + 命令 -\u0026gt; 重放\nstate machine replication例子\nmysql主从复制 slave relay log, 基于sql语句的复制[9];\nredis AOF\nprimary-copy例子:\nzookeeper的主从复制;\nmysql主从复制 slave relay log, 基于行的复制[9];\nredis RDB 快照;\n参考 # 一致性 # 保证分布式系统数据一致性的6种方案 高可用架构 *** 深入解析NoSQL数据库的分布式算法 *** ZooKeeper真不是最终一致性的，而是顺序一致性 陈东明 为什么程序员需要关心顺序一致性（Sequential Consistency）而不是Cache一致性（Cache Coherence） carlosstephen 分布式系统：一致性模型 阿里 Overview *** ENode 1.0 - Saga的思想与实现 汤雪华 《大数据日知录：架构与算法》 张俊林 Base: An Acid Alternative Ebay模式 *** 如何选择分布式事务解决方案？ ali *** {% post_link \u0026lsquo;NoSQL\u0026rsquo; %} self 一致性 （建议收藏）万字长文总结分布式事务，总有一款适合你 *** 腾讯 《数据密集型应用系统设计》笔记五：第五章 数据复制 未 应用 # 数据一致性检测应用场景与最佳实践 阿里 未 向量时钟 # 向量时钟Vector Clock in Riak Why Vector Clocks Are Hard 未 Dynamo: Amazon’s Highly Available Key-value Store paper 未\n向量时钟的变种 版本向量（Version vector） 版本控制机制 分布式系统：向量时钟 阿里 肖汉松 *** "},{"id":45,"href":"/www6vDistributed/docs/%E4%B8%80%E8%87%B4%E6%80%A7/%E5%88%86%E5%B8%83%E5%BC%8F%E5%8D%8F%E8%AE%AE/zookeeperZab/","title":"Zookeeper-Zab","section":"分布式协议","content":"\nZab协议 # Overview [1] # ZAB的四个阶段 [3] # 后三个阶段 [5] # 阶段一 选举阶段（Leader Election） [4] # 成为Leader的条件：\n选epoch最大的 epoch相等，选zxid最大的 epoch和zxid都相等，选server_id最大的（zoo.cfg 中配置的 myid） 服务器状态\nLOOKING 不确定Leader状态。该状态下的服务器认为当前集群中没有Leader，会发起Leader选举 FOLLOWING 跟随者状态。表明当前服务器角色是Follower，并且它知道Leader是谁 LEADING 领导者状态。表明当前服务器角色是Leader，它会维护与Follower间的心跳 OBSERVING 观察者状态。表明当前服务器角色是Observer，与Folower唯一的不同在于不参与选举，也不参与集群写操作时的投票 快速选举（Fast Leader Election） 节点在选举开始时，都默认投票给自己，当接收其他节点的选票时，会根据上面的 Leader条件 判断并且更改自己的选票，然后重新发送选票给其他节点。当有一个节点的得票超过半数，该节点会设置自己的状态为 Leading ，其他节点会设置自己的状态为 Following。\n阶段二 发现阶段 # 在这个阶段中，Followers和上一轮选举出的准Leader进行通信，同步Followers最近接受的事务Proposal。这个阶段主要目的是发现当前大多数节点接受的最新提议，并且准Leader生成新的epoch，让Followers接受，更新它们的acceptedEpoch。\n一个Follower只会连接一个Leader，如果有一个节点F认为另一个Follower P是Leader，F在尝试连接P时会被拒绝，F被拒绝后，就会进入选举阶段。\n阶段三 同步阶段 # 同步阶段主要是利用 Leader 前一阶段获得的最新 Proposal 历史，同步集群中所有的副本。\n只有当 quorum（超过半数的节点） 都同步完成，准 Leader 才会成为真正的 Leader。Follower 只会接收 zxid 比自己 lastZxid 大的 Proposal。\n阶段四 广播阶段 # 到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 Leader 可以进行消息广播。同时，如果有新的节点加入，还需要对新节点进行同步。 需要注意的是，Zab 提交事务并不像 2PC 一样需要全部 Follower 都 Ack，只需要得到 quorum（超过半数的节点）的Ack 就可以。\n# 参考 # 《PAXOS到ZOOKEEPER分布式一致性原理与实践》 第4章 倪超 分布式服务框架 Zookeeper — 管理分布式环境中的数据 失效 Zookeeper一致性协议——ZAB *** 深入浅出Zookeeper（一） Zookeeper架构及FastLeaderElection机制 *** 分布式一致性协议 - ZAB Zab: High-performance broadcast for primary-backup systems paper 未\n"}]